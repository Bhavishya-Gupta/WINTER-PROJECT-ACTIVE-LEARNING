{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Define dataset transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))  # Normalize to [-1, 1]\n",
        "])\n",
        "\n",
        "# Download and load the FashionMNIST dataset\n",
        "trainset = torchvision.datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=32, shuffle=True)\n",
        "\n",
        "testset = torchvision.datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=32, shuffle=False)\n",
        "\n",
        "classes = [\n",
        "    'T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
        "    'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot'\n",
        "]\n",
        "\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)  # Changed input channels to 1 for grayscale\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.fc1 = nn.Linear(64 * 7 * 7, 128)  # Adjusted for FashionMNIST input size (28x28)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(self.relu(self.conv1(x)))\n",
        "        x = self.pool(self.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 64 * 7 * 7)\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Initialize the model\n",
        "model = CNN()\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 5\n",
        "for epoch in range(num_epochs):\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        inputs, labels = data\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        if i % 100 == 99:  # Print every 100 mini-batches\n",
        "            print(f\"[{epoch + 1}, {i + 1}] loss: {running_loss / 100:.3f}\")\n",
        "            running_loss = 0.0\n",
        "\n",
        "print(\"Finished Training\")\n",
        "# Evaluate the model\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f\"Accuracy on test set: {100 * correct / total:.2f}%\")\n",
        "# Fine-tuning a pretrained model\n",
        "from torchvision import models\n",
        "\n",
        "pretrained_model = models.resnet18(pretrained=True)\n",
        "pretrained_model.fc = nn.Linear(pretrained_model.fc.in_features, 10)\n",
        "\n",
        "# Freeze layers except the final fully connected layer\n",
        "for param in pretrained_model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "for param in pretrained_model.fc.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "# Define optimizer and criterion for pretrained model\n",
        "optimizer = optim.Adam(pretrained_model.fc.parameters(), lr=0.001)\n",
        "\n",
        "# Similar training loop can be implemented as before for fine-tuning\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9AA-FLSX_VEl",
        "outputId": "6eff121e-f5cf-4c22-ab8f-7aa561addaf3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26.4M/26.4M [00:01<00:00, 13.3MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 29.5k/29.5k [00:00<00:00, 212kB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4.42M/4.42M [00:03<00:00, 1.41MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5.15k/5.15k [00:00<00:00, 5.30MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n",
            "[1, 100] loss: 0.979\n",
            "[1, 200] loss: 0.553\n",
            "[1, 300] loss: 0.488\n",
            "[1, 400] loss: 0.438\n",
            "[1, 500] loss: 0.422\n",
            "[1, 600] loss: 0.376\n",
            "[1, 700] loss: 0.392\n",
            "[1, 800] loss: 0.379\n",
            "[1, 900] loss: 0.373\n",
            "[1, 1000] loss: 0.356\n",
            "[1, 1100] loss: 0.339\n",
            "[1, 1200] loss: 0.351\n",
            "[1, 1300] loss: 0.314\n",
            "[1, 1400] loss: 0.309\n",
            "[1, 1500] loss: 0.303\n",
            "[1, 1600] loss: 0.321\n",
            "[1, 1700] loss: 0.293\n",
            "[1, 1800] loss: 0.297\n",
            "[2, 100] loss: 0.258\n",
            "[2, 200] loss: 0.261\n",
            "[2, 300] loss: 0.276\n",
            "[2, 400] loss: 0.292\n",
            "[2, 500] loss: 0.275\n",
            "[2, 600] loss: 0.244\n",
            "[2, 700] loss: 0.257\n",
            "[2, 800] loss: 0.258\n",
            "[2, 900] loss: 0.238\n",
            "[2, 1000] loss: 0.247\n",
            "[2, 1100] loss: 0.244\n",
            "[2, 1200] loss: 0.248\n",
            "[2, 1300] loss: 0.243\n",
            "[2, 1400] loss: 0.266\n",
            "[2, 1500] loss: 0.260\n",
            "[2, 1600] loss: 0.237\n",
            "[2, 1700] loss: 0.262\n",
            "[2, 1800] loss: 0.256\n",
            "[3, 100] loss: 0.204\n",
            "[3, 200] loss: 0.216\n",
            "[3, 300] loss: 0.215\n",
            "[3, 400] loss: 0.212\n",
            "[3, 500] loss: 0.191\n",
            "[3, 600] loss: 0.216\n",
            "[3, 700] loss: 0.206\n",
            "[3, 800] loss: 0.221\n",
            "[3, 900] loss: 0.209\n",
            "[3, 1000] loss: 0.178\n",
            "[3, 1100] loss: 0.214\n",
            "[3, 1200] loss: 0.216\n",
            "[3, 1300] loss: 0.218\n",
            "[3, 1400] loss: 0.208\n",
            "[3, 1500] loss: 0.208\n",
            "[3, 1600] loss: 0.215\n",
            "[3, 1700] loss: 0.202\n",
            "[3, 1800] loss: 0.182\n",
            "[4, 100] loss: 0.163\n",
            "[4, 200] loss: 0.167\n",
            "[4, 300] loss: 0.184\n",
            "[4, 400] loss: 0.165\n",
            "[4, 500] loss: 0.176\n",
            "[4, 600] loss: 0.186\n",
            "[4, 700] loss: 0.169\n",
            "[4, 800] loss: 0.183\n",
            "[4, 900] loss: 0.188\n",
            "[4, 1000] loss: 0.178\n",
            "[4, 1100] loss: 0.178\n",
            "[4, 1200] loss: 0.174\n",
            "[4, 1300] loss: 0.182\n",
            "[4, 1400] loss: 0.179\n",
            "[4, 1500] loss: 0.171\n",
            "[4, 1600] loss: 0.161\n",
            "[4, 1700] loss: 0.164\n",
            "[4, 1800] loss: 0.172\n",
            "[5, 100] loss: 0.155\n",
            "[5, 200] loss: 0.138\n",
            "[5, 300] loss: 0.142\n",
            "[5, 400] loss: 0.159\n",
            "[5, 500] loss: 0.135\n",
            "[5, 600] loss: 0.151\n",
            "[5, 700] loss: 0.137\n",
            "[5, 800] loss: 0.142\n",
            "[5, 900] loss: 0.152\n",
            "[5, 1000] loss: 0.158\n",
            "[5, 1100] loss: 0.150\n",
            "[5, 1200] loss: 0.159\n",
            "[5, 1300] loss: 0.151\n",
            "[5, 1400] loss: 0.151\n",
            "[5, 1500] loss: 0.143\n",
            "[5, 1600] loss: 0.140\n",
            "[5, 1700] loss: 0.152\n",
            "[5, 1800] loss: 0.149\n",
            "Finished Training\n",
            "Accuracy on test set: 91.63%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 146MB/s]\n"
          ]
        }
      ]
    }
  ]
}