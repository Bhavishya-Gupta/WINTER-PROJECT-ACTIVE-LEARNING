{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b0aa17f-921f-400f-9a95-c22a6b4057c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets,transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "967d4057-2aa2-4e69-8cd2-d18846cf6c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf86adf2-07fa-4eaa-9adf-728c8fbadf7a",
   "metadata": {},
   "source": [
    "### CNN Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "246e4842-6af9-4e45-90ad-ea321f72aae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionalNeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(10, 25, 3, 1)\n",
    "\n",
    "        self.fc1 = nn.Linear(25*6*6, 120)\n",
    "        self.fc2 = nn.Linear(120, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(F.max_pool2d(x, 2, 2))\n",
    "        x = F.relu(F.max_pool2d(x, 2, 2))\n",
    "\n",
    "        x = x.view(-1, 25*6*6)\n",
    "\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9001e70-0cb9-475a-9085-c88a0a493484",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvolutionalNeuralNetwork(\n",
       "  (conv1): Conv2d(1, 10, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (conv2): Conv2d(10, 25, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (fc1): Linear(in_features=900, out_features=120, bias=True)\n",
       "  (fc2): Linear(in_features=120, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ConvolutionalNeuralNetwork()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c86a443-e5f1-4689-800e-5ab48fad0f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b7fb26-53ae-47c9-9352-86ef60052412",
   "metadata": {},
   "source": [
    "### Dataset Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28f6f580-473c-4e85-8a3f-a57e10aacd51",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.ToTensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f284a0f2-a2f8-43b4-875b-ae59163f7c6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to cnn_data\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting cnn_data\\MNIST\\raw\\train-images-idx3-ubyte.gz to cnn_data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to cnn_data\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting cnn_data\\MNIST\\raw\\train-labels-idx1-ubyte.gz to cnn_data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to cnn_data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting cnn_data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to cnn_data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to cnn_data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting cnn_data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to cnn_data\\MNIST\\raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_data = datasets.MNIST(root = 'cnn_data', train = True, download = True, transform = transform)\n",
    "test_data = datasets.MNIST(root = 'cnn_data', train = False, download = True, transform = transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f195a9b-fd64-4424-b115-047810e6f124",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_data, batch_size = 10, shuffle = True)\n",
    "test_loader = DataLoader(test_data, batch_size = 10, shuffle = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9988e441-e021-4040-bfff-fce607b8a1df",
   "metadata": {},
   "source": [
    "### Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "144b6f3a-7f80-4f8d-969b-60cd043006f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1     loss: 0.17247\n",
      "epoch: 2     loss: 0.12164\n",
      "epoch: 3     loss: 0.10716\n",
      "epoch: 4     loss: 0.10311\n",
      "epoch: 5     loss: 0.10231\n",
      "epoch: 6     loss: 0.09960\n",
      "epoch: 7     loss: 0.09501\n",
      "epoch: 8     loss: 0.09036\n",
      "epoch: 9     loss: 0.09097\n",
      "epoch: 10     loss: 0.09090\n"
     ]
    }
   ],
   "source": [
    "test_correct = 0\n",
    "\n",
    "for epoch in range(10):\n",
    "    total_loss = 0\n",
    "    for i, (X_train, y_train) in enumerate(train_loader):\n",
    "        y_prediction = model(X_train)\n",
    "        loss = loss_function(y_prediction, y_train)\n",
    "        total_loss += loss\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'epoch: {epoch + 1}     loss: {total_loss / len(train_loader) :.5f}')\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, (X_test, y_test) in enumerate(test_loader):\n",
    "        y_value = model(X_test)\n",
    "        loss = loss_function(y_value, y_test)\n",
    "\n",
    "        test_correct += (torch.max(y_value, 1)[1] == y_test).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d1b2870d-5b5a-4f67-8f7e-ec4896fcbdc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the Test Data: 96.60%\n"
     ]
    }
   ],
   "source": [
    "print(f'Accuracy on the Test Data: {test_correct/len(test_data)*100 :.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d37719-d523-4da0-934e-6bb1d97254e4",
   "metadata": {},
   "source": [
    "### Pretrained CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c7ec6a9a-5b93-45f1-8336-29f2253a9871",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torchvision import models\n",
    "\n",
    "pretrained_model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "\n",
    "pretrained_model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "\n",
    "pretrained_model.fc = nn.Linear(pretrained_model.fc.in_features, 10)\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(pretrained_model.parameters(), lr=0.0001)\n",
    "\n",
    "for param in pretrained_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for param in pretrained_model.fc.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a0c098e-a3fd-449f-9535-03c7329e9077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1     loss: 1.65961\n",
      "epoch: 2     loss: 1.41161\n",
      "epoch: 3     loss: 1.36473\n",
      "epoch: 4     loss: 1.34735\n",
      "epoch: 5     loss: 1.32731\n",
      "epoch: 6     loss: 1.32807\n",
      "epoch: 7     loss: 1.32147\n",
      "epoch: 8     loss: 1.31355\n",
      "epoch: 9     loss: 1.30999\n",
      "epoch: 10     loss: 1.31081\n"
     ]
    }
   ],
   "source": [
    "test_correct = 0\n",
    "\n",
    "for epoch in range(10):\n",
    "    total_loss = 0\n",
    "    for i, (X_train, y_train) in enumerate(train_loader):\n",
    "        y_prediction = pretrained_model(X_train)\n",
    "        loss = loss_function(y_prediction, y_train)\n",
    "        total_loss += loss\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'epoch: {epoch + 1}     loss: {total_loss / len(train_loader) :.5f}')\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, (X_test, y_test) in enumerate(test_loader):\n",
    "        y_value = pretrained_model(X_test)\n",
    "        loss = loss_function(y_value, y_test)\n",
    "\n",
    "        test_correct += (torch.max(y_value, 1)[1] == y_test).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aa92b5d1-4a14-4251-8df6-f5947875d590",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the Test Data: 57.65%\n"
     ]
    }
   ],
   "source": [
    "print(f'Accuracy on the Test Data: {test_correct/len(test_data)*100 :.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
