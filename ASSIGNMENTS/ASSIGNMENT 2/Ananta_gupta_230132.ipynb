{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on initial labeled dataset...\n",
      "Epoch 1/10, Loss: 1.5993\n",
      "Epoch 2/10, Loss: 0.8649\n",
      "Epoch 3/10, Loss: 0.7129\n",
      "Epoch 4/10, Loss: 0.6196\n",
      "Epoch 5/10, Loss: 0.5537\n",
      "Epoch 6/10, Loss: 0.5157\n",
      "Epoch 7/10, Loss: 0.4797\n",
      "Epoch 8/10, Loss: 0.4262\n",
      "Epoch 9/10, Loss: 0.4246\n",
      "Epoch 10/10, Loss: 0.4207\n",
      "Initial Test Accuracy: 80.05%\n",
      "\n",
      "Active Learning Iteration 1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import numpy as np\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 64 * 7 * 7)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def prepare_data():\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,))\n",
    "    ])\n",
    "    train_dataset = datasets.FashionMNIST(root='./data', train=True, transform=transform, download=True)\n",
    "    test_dataset = datasets.FashionMNIST(root='./data', train=False, transform=transform, download=True)\n",
    "    return train_dataset, test_dataset\n",
    "\n",
    "def split_dataset(dataset, initial_labeled_size):\n",
    "    indices = list(range(len(dataset)))\n",
    "    labeled_indices = np.random.choice(indices, size=initial_labeled_size, replace=False)\n",
    "    unlabeled_indices = [i for i in indices if i not in labeled_indices]\n",
    "    return Subset(dataset, labeled_indices), Subset(dataset, unlabeled_indices)\n",
    "\n",
    "\n",
    "def train_model(model, data_loader, optimizer, criterion, epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for images, labels in data_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss / len(data_loader):.4f}\")\n",
    "\n",
    "\n",
    "def evaluate_model(model, data_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in data_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy\n",
    "\n",
    "# Active learning: uncertainty-based sample selection\n",
    "def get_uncertain_samples(model, data_loader, num_samples, strategy=\"entropy\"):\n",
    "    model.eval()\n",
    "    uncertainties = []\n",
    "    with torch.no_grad():\n",
    "        for images, indices in data_loader:\n",
    "            images = images.to(device)\n",
    "            outputs = model(images)\n",
    "            probs = F.softmax(outputs, dim=1)\n",
    "            \n",
    "            if strategy == \"entropy\":\n",
    "                entropy = -torch.sum(probs * torch.log(probs + 1e-10), dim=1)\n",
    "                uncertainties.extend(zip(entropy.tolist(), indices.tolist()))\n",
    "            elif strategy == \"least_confidence\":\n",
    "                confidence, _ = torch.max(probs, dim=1)\n",
    "                uncertainties.extend(zip(-confidence.tolist(), indices.tolist()))\n",
    "            elif strategy == \"margin\":\n",
    "                sorted_probs, _ = probs.sort(dim=1, descending=True)\n",
    "                margin = sorted_probs[:, 0] - sorted_probs[:, 1]\n",
    "                uncertainties.extend(zip(-margin.tolist(), indices.tolist()))\n",
    "    \n",
    "    uncertainties.sort(reverse=True, key=lambda x: x[0])\n",
    "    return [index for _, index in uncertainties[:num_samples]]\n",
    "\n",
    "\n",
    "def get_diverse_samples(model, data_loader, num_samples, diversity_metric=\"cosine_similarity\"):\n",
    "    model.eval()\n",
    "    features_list, indices = [], []\n",
    "    with torch.no_grad():\n",
    "        for images, img_indices in data_loader:\n",
    "            images = images.to(device)\n",
    "            outputs = model(images)\n",
    "            \n",
    "            # Apply global average pooling instead of full feature map\n",
    "            features = F.adaptive_avg_pool2d(outputs, (1, 1)).view(outputs.size(0), -1)\n",
    "            features_list.append(features)\n",
    "            indices.extend(img_indices)\n",
    "\n",
    "    features = torch.cat(features_list, dim=0)\n",
    "\n",
    "    if diversity_metric == \"cosine_similarity\":\n",
    "        similarity_matrix = F.cosine_similarity(features.unsqueeze(1), features.unsqueeze(0), dim=2)\n",
    "        uncertainties = 1 - similarity_matrix.max(dim=1)[0]\n",
    "    elif diversity_metric == \"l2_norm\":\n",
    "        center = features.mean(dim=0)\n",
    "        distances = torch.norm(features - center, dim=1)\n",
    "        uncertainties = distances\n",
    "    elif diversity_metric == \"kl_divergence\":\n",
    "        probs = F.softmax(model(features), dim=1)\n",
    "        entropy = -torch.sum(probs * torch.log(probs + 1e-10), dim=1)\n",
    "        uncertainties = entropy\n",
    "\n",
    "    uncertainties, indices = zip(*sorted(zip(uncertainties, indices), reverse=True, key=lambda x: x[0]))\n",
    "    return list(indices[:num_samples])\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    " \n",
    "    train_dataset, test_dataset = prepare_data()\n",
    "    initial_labeled_size = 1000\n",
    "    labeled_set, unlabeled_set = split_dataset(train_dataset, initial_labeled_size)\n",
    "\n",
    "    labeled_loader = DataLoader(labeled_set, batch_size=64, shuffle=True)\n",
    "    unlabeled_loader = DataLoader(unlabeled_set, batch_size=64, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "    # Initialize model, loss, and optimizer\n",
    "    model = CNN().to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    # Train initial model on labeled set\n",
    "    print(\"Training on initial labeled dataset...\")\n",
    "    train_model(model, labeled_loader, optimizer, criterion, epochs=10)\n",
    "\n",
    "    # Evaluate on test set\n",
    "    accuracy = evaluate_model(model, test_loader)\n",
    "    print(f\"Initial Test Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "    # Active learning iterations\n",
    "    num_iterations = 5\n",
    "    num_samples = 100\n",
    "\n",
    "    for iteration in range(num_iterations):\n",
    "        print(f\"\\nActive Learning Iteration {iteration + 1}\")\n",
    "        \n",
    "        # Select uncertain samples\n",
    "        new_indices_uncertainty = get_uncertain_samples(model, unlabeled_loader, num_samples, strategy=\"entropy\")\n",
    "        \n",
    "        # Select diverse samples\n",
    "        new_indices_diversity = get_diverse_samples(model, unlabeled_loader, num_samples, diversity_metric=\"cosine_similarity\")\n",
    "\n",
    "        # Combine indices and remove duplicates\n",
    "        new_indices = list(set(new_indices_uncertainty + new_indices_diversity))\n",
    "\n",
    "        # Convert labeled_set.indices to a list and update subsets\n",
    "        labeled_set_indices = list(labeled_set.indices)\n",
    "        labeled_set_indices.extend(new_indices)\n",
    "        unlabeled_set_indices = [i for i in unlabeled_set.indices if i not in new_indices]\n",
    "\n",
    "        # Update labeled and unlabeled subsets\n",
    "        labeled_set = Subset(train_dataset, labeled_set_indices)\n",
    "        unlabeled_set = Subset(train_dataset, unlabeled_set_indices)\n",
    "\n",
    "        # Update data loaders\n",
    "        labeled_loader = DataLoader(labeled_set, batch_size=64, shuffle=True)\n",
    "        unlabeled_loader = DataLoader(unlabeled_set, batch_size=64, shuffle=False)\n",
    "\n",
    "        # Retrain the model\n",
    "        train_model(model, labeled_loader, optimizer, criterion, epochs=5)\n",
    "\n",
    "        # Evaluate on test set\n",
    "        accuracy = evaluate_model(model, test_loader)\n",
    "        print(f\"Iteration {iteration + 1} Test Accuracy: {accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
