{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms, models\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from scipy.spatial.distance import cdist\n",
        "\n",
        "# Define Active Learning Strategies\n",
        "def least_confidence(model, data_loader, device):\n",
        "    model.eval()\n",
        "    uncertainties = []\n",
        "    with torch.no_grad():\n",
        "        for images, _ in data_loader:\n",
        "            images = images.to(device)\n",
        "            outputs = model(images)\n",
        "            probs = F.softmax(outputs, dim=1)\n",
        "            uncertainties.extend(1 - torch.max(probs, dim=1)[0].cpu().numpy())\n",
        "    return np.argsort(uncertainties)[::-1]  # Sort by least confidence\n",
        "\n",
        "def prediction_entropy(model, data_loader, device):\n",
        "    model.eval()\n",
        "    entropies = []\n",
        "    with torch.no_grad():\n",
        "        for images, _ in data_loader:\n",
        "            images = images.to(device)\n",
        "            outputs = model(images)\n",
        "            probs = F.softmax(outputs, dim=1)\n",
        "            entropy = -torch.sum(probs * torch.log(probs + 1e-6), dim=1).cpu().numpy()\n",
        "            entropies.extend(entropy)\n",
        "    return np.argsort(entropies)[::-1]  # Sort by highest entropy\n",
        "\n",
        "def margin_sampling(model, data_loader, device):\n",
        "    model.eval()\n",
        "    margins = []\n",
        "    with torch.no_grad():\n",
        "        for images, _ in data_loader:\n",
        "            images = images.to(device)\n",
        "            outputs = model(images)\n",
        "            probs = F.softmax(outputs, dim=1)\n",
        "            sorted_probs, _ = torch.sort(probs, descending=True)\n",
        "            margin = sorted_probs[:, 0] - sorted_probs[:, 1]\n",
        "            margins.extend(margin.cpu().numpy())\n",
        "    return np.argsort(margins)[::-1]  # Sort by largest margin\n",
        "\n",
        "def cosine_similarity_selection(model, data_loader, device, labeled_data):\n",
        "    model.eval()\n",
        "    labeled_embeddings = []\n",
        "    with torch.no_grad():\n",
        "        for images, _ in labeled_data:\n",
        "            images = images.to(device)\n",
        "            outputs = model(images)\n",
        "            labeled_embeddings.append(outputs.cpu().numpy())\n",
        "    labeled_embeddings = np.concatenate(labeled_embeddings, axis=0)\n",
        "\n",
        "    similarities = []\n",
        "    with torch.no_grad():\n",
        "        for images, _ in data_loader:\n",
        "            images = images.to(device)\n",
        "            outputs = model(images)\n",
        "            embeddings = outputs.cpu().numpy()\n",
        "            sim = cosine_similarity(embeddings, labeled_embeddings)\n",
        "            similarities.extend(np.max(sim, axis=1))\n",
        "    return np.argsort(similarities)[::-1]  # Sort by highest cosine similarity\n",
        "\n",
        "def l2_norm_selection(model, data_loader, device, labeled_data):\n",
        "    model.eval()\n",
        "    labeled_embeddings = []\n",
        "    with torch.no_grad():\n",
        "        for images, _ in labeled_data:\n",
        "            images = images.to(device)\n",
        "            outputs = model(images)\n",
        "            labeled_embeddings.append(outputs.cpu().numpy())\n",
        "    labeled_embeddings = np.concatenate(labeled_embeddings, axis=0)\n",
        "\n",
        "    distances = []\n",
        "    with torch.no_grad():\n",
        "        for images, _ in data_loader:\n",
        "            images = images.to(device)\n",
        "            outputs = model(images)\n",
        "            embeddings = outputs.cpu().numpy()\n",
        "            dist = cdist(embeddings, labeled_embeddings, 'euclidean')\n",
        "            distances.extend(np.min(dist, axis=1))\n",
        "    return np.argsort(distances)  # Sort by smallest L2 distance\n",
        "\n",
        "def kl_divergence_selection(model, data_loader, device):\n",
        "    model.eval()\n",
        "    divergences = []\n",
        "    with torch.no_grad():\n",
        "        for images, _ in data_loader:\n",
        "            images = images.to(device)\n",
        "            outputs = model(images)\n",
        "            probs = F.softmax(outputs, dim=1)\n",
        "            log_probs = F.log_softmax(outputs, dim=1)\n",
        "            divergence = torch.sum(probs * (torch.log(probs + 1e-6) - log_probs), dim=1).cpu().numpy()\n",
        "            divergences.extend(divergence)\n",
        "    return np.argsort(divergences)[::-1]  # Sort by largest KL divergence\n",
        "\n",
        "\n",
        "# Initialize models, criterion, optimizer\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "custom_model = CustomCNN(num_conv_layers=2, filter_size=16).to(device)\n",
        "pretrained_model = models.resnet18(pretrained=True)\n",
        "pretrained_model.fc = nn.Linear(pretrained_model.fc.in_features, 10)\n",
        "pretrained_model = pretrained_model.to(device)\n",
        "\n",
        "# Loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "custom_optimizer = optim.Adam(custom_model.parameters(), lr=1e-3)\n",
        "pretrained_optimizer = optim.Adam(pretrained_model.parameters(), lr=1e-3)\n",
        "\n",
        "# Active learning loop with strategy tracking\n",
        "num_iterations = 5\n",
        "samples_per_iteration = 1000\n",
        "strategy_performance = {  # Track the average accuracy per strategy\n",
        "    'Least Confidence': [],\n",
        "    'Prediction Entropy': [],\n",
        "    'Margin Sampling': [],\n",
        "    'Cosine Similarity': [],\n",
        "    'L2 Norm': [],\n",
        "    'KL Divergence': []\n",
        "}\n",
        "\n",
        "# Function to train a model\n",
        "def train_model(model, data_loader, criterion, optimizer, device):\n",
        "    model.train()  # Set model to training mode\n",
        "    running_loss = 0.0\n",
        "    for images, labels in data_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        # Zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "\n",
        "        # Calculate loss\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass and optimize\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Accumulate loss\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    # Return average loss for this epoch\n",
        "    epoch_loss = running_loss / len(data_loader)\n",
        "    return epoch_loss\n",
        "\n",
        "# Function to test the model\n",
        "def test_model(model, test_loader, device):\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    return accuracy\n",
        "\n",
        "# Active learning loop\n",
        "for iteration in range(num_iterations):\n",
        "    print(f\"\\nActive Learning Iteration {iteration + 1}/{num_iterations}\")\n",
        "\n",
        "    # Train custom model\n",
        "    for epoch in range(5):\n",
        "        train_loss = train_model(custom_model, labeled_loader, criterion, custom_optimizer, device)\n",
        "        print(f\"Custom Model - Iteration {iteration + 1}, Epoch {epoch + 1}, Loss: {train_loss:.4f}\")\n",
        "\n",
        "    custom_accuracy = test_model(custom_model, test_loader, device)\n",
        "    print(f\"Custom Model Test Accuracy after Iteration {iteration + 1}: {custom_accuracy:.2f}%\")\n",
        "\n",
        "    # Train pretrained model\n",
        "    for epoch in range(5):\n",
        "        train_loss = train_model(pretrained_model, labeled_loader, criterion, pretrained_optimizer, device)\n",
        "        print(f\"Pretrained Model - Iteration {iteration + 1}, Epoch {epoch + 1}, Loss: {train_loss:.4f}\")\n",
        "\n",
        "    pretrained_accuracy = test_model(pretrained_model, test_loader, device)\n",
        "    print(f\"Pretrained Model Test Accuracy after Iteration {iteration + 1}: {pretrained_accuracy:.2f}%\")\n",
        "\n",
        "    # Active learning strategy\n",
        "    if len(unlabeled_dataset) > 0:\n",
        "        unlabeled_loader = DataLoader(unlabeled_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "        # Track accuracy for each strategy\n",
        "        strategy_results = {}\n",
        "\n",
        "        strategies = [\n",
        "            ('Least Confidence', least_confidence),\n",
        "            ('Prediction Entropy', prediction_entropy),\n",
        "            ('Margin Sampling', margin_sampling),\n",
        "            ('Cosine Similarity', cosine_similarity_selection),\n",
        "            ('L2 Norm', l2_norm_selection),\n",
        "            ('KL Divergence', kl_divergence_selection)\n",
        "        ]\n",
        "\n",
        "        for strategy_name, strategy_fn in strategies:\n",
        "            print(f\"Applying {strategy_name}...\")\n",
        "\n",
        "            selected_indices = strategy_fn(custom_model, unlabeled_loader, device, labeled_loader)\n",
        "            selected_indices = selected_indices[:samples_per_iteration]\n",
        "\n",
        "            labeled_indices = np.append(labeled_indices, unlabeled_indices[selected_indices])\n",
        "            unlabeled_indices = np.setdiff1d(unlabeled_indices, unlabeled_indices[selected_indices])\n",
        "\n",
        "            # Update datasets\n",
        "            labeled_dataset = Subset(train_dataset, labeled_indices)\n",
        "            unlabeled_dataset = Subset(train_dataset, unlabeled_indices)\n",
        "            labeled_loader = DataLoader(labeled_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "            # Test the model after applying this strategy\n",
        "            accuracy = test_model(custom_model, test_loader, device)\n",
        "            strategy_results[strategy_name] = accuracy\n",
        "            print(f\"{strategy_name} Test Accuracy: {accuracy:.2f}%\")\n",
        "\n",
        "        # Track performance for each strategy over all iterations\n",
        "        for strategy_name, accuracy in strategy_results.items():\n",
        "            strategy_performance[strategy_name].append(accuracy)\n",
        "\n",
        "# Compute average accuracy for each strategy across all iterations\n",
        "average_accuracy_per_strategy = {strategy: np.mean(accuracies) for strategy, accuracies in strategy_performance.items()}\n",
        "\n",
        "# Determine the most effective strategy overall\n",
        "most_effective_strategy = max(average_accuracy_per_strategy, key=average_accuracy_per_strategy.get)\n",
        "print(f\"\\nThe Most Effective Active Learning Strategy Overall: {most_effective_strategy}\")\n"
      ],
      "metadata": {
        "id": "pxPWOkSgg7Xg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "outputId": "331fef1b-435d-4cc5-b677-5646817f3946"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'CustomCNN' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-ca0e75659453>\u001b[0m in \u001b[0;36m<cell line: 104>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;31m# Initialize models, criterion, optimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m \u001b[0mcustom_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCustomCNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_conv_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0mpretrained_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresnet18\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0mpretrained_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'CustomCNN' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms, models\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from scipy.spatial.distance import cdist\n",
        "\n",
        "# Define the CustomCNN class\n",
        "class CustomCNN(nn.Module):\n",
        "    def __init__(self, num_conv_layers=2, filter_size=16, num_classes=10):\n",
        "        super(CustomCNN, self).__init__()\n",
        "        layers = []\n",
        "        in_channels = 3  # Assuming input images are RGB\n",
        "        for _ in range(num_conv_layers):\n",
        "            layers.append(nn.Conv2d(in_channels, filter_size, kernel_size=3, padding=1))\n",
        "            layers.append(nn.ReLU())\n",
        "            layers.append(nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "            in_channels = filter_size\n",
        "            filter_size *= 2  # Increase filter size in each layer\n",
        "        self.conv = nn.Sequential(*layers)\n",
        "        self.fc = nn.Linear(in_channels * 8 * 8, num_classes)  # Adjust for input size (32x32 for CIFAR-10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = torch.flatten(x, start_dim=1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "# Define Active Learning Strategies\n",
        "def least_confidence(model, data_loader, device):\n",
        "    model.eval()\n",
        "    uncertainties = []\n",
        "    with torch.no_grad():\n",
        "        for images, _ in data_loader:\n",
        "            images = images.to(device)\n",
        "            outputs = model(images)\n",
        "            probs = F.softmax(outputs, dim=1)\n",
        "            uncertainties.extend(1 - torch.max(probs, dim=1)[0].cpu().numpy())\n",
        "    return np.argsort(uncertainties)[::-1]\n",
        "\n",
        "def prediction_entropy(model, data_loader, device):\n",
        "    model.eval()\n",
        "    entropies = []\n",
        "    with torch.no_grad():\n",
        "        for images, _ in data_loader:\n",
        "            images = images.to(device)\n",
        "            outputs = model(images)\n",
        "            probs = F.softmax(outputs, dim=1)\n",
        "            entropy = -torch.sum(probs * torch.log(probs + 1e-6), dim=1).cpu().numpy()\n",
        "            entropies.extend(entropy)\n",
        "    return np.argsort(entropies)[::-1]\n",
        "\n",
        "def margin_sampling(model, data_loader, device):\n",
        "    model.eval()\n",
        "    margins = []\n",
        "    with torch.no_grad():\n",
        "        for images, _ in data_loader:\n",
        "            images = images.to(device)\n",
        "            outputs = model(images)\n",
        "            probs = F.softmax(outputs, dim=1)\n",
        "            sorted_probs, _ = torch.sort(probs, descending=True)\n",
        "            margin = sorted_probs[:, 0] - sorted_probs[:, 1]\n",
        "            margins.extend(margin.cpu().numpy())\n",
        "    return np.argsort(margins)[::-1]\n",
        "\n",
        "def cosine_similarity_selection(model, data_loader, device, labeled_data):\n",
        "    model.eval()\n",
        "    labeled_embeddings = []\n",
        "    with torch.no_grad():\n",
        "        for images, _ in labeled_data:\n",
        "            images = images.to(device)\n",
        "            outputs = model(images)\n",
        "            labeled_embeddings.append(outputs.cpu().numpy())\n",
        "    labeled_embeddings = np.concatenate(labeled_embeddings, axis=0)\n",
        "\n",
        "    similarities = []\n",
        "    with torch.no_grad():\n",
        "        for images, _ in data_loader:\n",
        "            images = images.to(device)\n",
        "            outputs = model(images)\n",
        "            embeddings = outputs.cpu().numpy()\n",
        "            sim = cosine_similarity(embeddings, labeled_embeddings)\n",
        "            similarities.extend(np.max(sim, axis=1))\n",
        "    return np.argsort(similarities)[::-1]\n",
        "\n",
        "def l2_norm_selection(model, data_loader, device, labeled_data):\n",
        "    model.eval()\n",
        "    labeled_embeddings = []\n",
        "    with torch.no_grad():\n",
        "        for images, _ in labeled_data:\n",
        "            images = images.to(device)\n",
        "            outputs = model(images)\n",
        "            labeled_embeddings.append(outputs.cpu().numpy())\n",
        "    labeled_embeddings = np.concatenate(labeled_embeddings, axis=0)\n",
        "\n",
        "    distances = []\n",
        "    with torch.no_grad():\n",
        "        for images, _ in data_loader:\n",
        "            images = images.to(device)\n",
        "            outputs = model(images)\n",
        "            embeddings = outputs.cpu().numpy()\n",
        "            dist = cdist(embeddings, labeled_embeddings, 'euclidean')\n",
        "            distances.extend(np.min(dist, axis=1))\n",
        "    return np.argsort(distances)\n",
        "\n",
        "def kl_divergence_selection(model, data_loader, device):\n",
        "    model.eval()\n",
        "    divergences = []\n",
        "    with torch.no_grad():\n",
        "        for images, _ in data_loader:\n",
        "            images = images.to(device)\n",
        "            outputs = model(images)\n",
        "            probs = F.softmax(outputs, dim=1)\n",
        "            log_probs = F.log_softmax(outputs, dim=1)\n",
        "            divergence = torch.sum(probs * (torch.log(probs + 1e-6) - log_probs), dim=1).cpu().numpy()\n",
        "            divergences.extend(divergence)\n",
        "    return np.argsort(divergences)[::-1]\n",
        "\n",
        "# Initialize models, criterion, optimizer\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "custom_model = CustomCNN(num_conv_layers=2, filter_size=16).to(device)\n",
        "pretrained_model = models.resnet18(pretrained=True)\n",
        "pretrained_model.fc = nn.Linear(pretrained_model.fc.in_features, 10)\n",
        "pretrained_model = pretrained_model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "custom_optimizer = optim.Adam(custom_model.parameters(), lr=1e-3)\n",
        "pretrained_optimizer = optim.Adam(pretrained_model.parameters(), lr=1e-3)\n",
        "\n",
        "# Dummy placeholders for DataLoader and Dataset (replace with your dataset)\n",
        "labeled_loader = None  # Replace with DataLoader for labeled data\n",
        "unlabeled_loader = None  # Replace with DataLoader for unlabeled data\n",
        "test_loader = None  # Replace with DataLoader for test data\n",
        "train_dataset = None  # Replace with your dataset\n",
        "labeled_indices = []  # List of indices for labeled data\n",
        "unlabeled_indices = []  # List of indices for unlabeled data\n",
        "\n",
        "# Active learning loop with strategy tracking\n",
        "num_iterations = 5\n",
        "samples_per_iteration = 1000\n",
        "strategy_performance = {\n",
        "    'Least Confidence': [],\n",
        "    'Prediction Entropy': [],\n",
        "    'Margin Sampling': [],\n",
        "    'Cosine Similarity': [],\n",
        "    'L2 Norm': [],\n",
        "    'KL Divergence': []\n",
        "}\n",
        "\n",
        "def train_model(model, data_loader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for images, labels in data_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "    return running_loss / len(data_loader)\n",
        "\n",
        "def test_model(model, test_loader, device):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    return 100 * correct / total\n",
        "\n",
        "for iteration in range(num_iterations):\n",
        "    print(f\"\\nActive Learning Iteration {iteration + 1}/{num_iterations}\")\n",
        "\n",
        "    # Train custom model\n",
        "    for epoch in range(5):\n",
        "        train_loss = train_model(custom_model, labeled_loader, criterion, custom_optimizer, device)\n",
        "        print(f\"Custom Model - Iteration {iteration + 1}, Epoch {epoch + 1}, Loss: {train_loss:.4f}\")\n",
        "\n",
        "    custom_accuracy = test_model(custom_model, test_loader, device)\n",
        "    print(f\"Custom Model Test Accuracy after Iteration {iteration + 1}: {custom_accuracy:.2f}%\")\n",
        "\n",
        "    # Train pretrained model\n",
        "    for epoch in range(5):\n",
        "        train_loss = train_model(pretrained_model, labeled_loader, criterion, pretrained_optimizer, device)\n",
        "        print(f\"Pretrained Model - Iteration {iteration + 1}, Epoch {epoch + 1}, Loss: {train_loss:.4f}\")\n",
        "\n",
        "    pretrained_accuracy = test_model(pretrained_model, test_loader, device)\n",
        "    print(f\"Pretrained Model Test Accuracy after Iteration {iteration + 1}: {pretrained_accuracy:.2f}%\")\n",
        "\n",
        "    # Active learning strategy (skipped due to placeholders for datasets)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 511
        },
        "id": "gSuQRAijCZ86",
        "outputId": "2aa89b3e-ffba-4cb8-95b6-2741304b1258"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 161MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Active Learning Iteration 1/5\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "'NoneType' object is not iterable",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-0469c1fab772>\u001b[0m in \u001b[0;36m<cell line: 179>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    182\u001b[0m     \u001b[0;31m# Train custom model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcustom_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabeled_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Custom Model - Iteration {iteration + 1}, Epoch {epoch + 1}, Loss: {train_loss:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-0469c1fab772>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, data_loader, criterion, optimizer, device)\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0mrunning_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m         \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms, models\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from scipy.spatial.distance import cdist\n",
        "\n",
        "# Define Active Learning Strategies\n",
        "def least_confidence(model, data_loader, device):\n",
        "    model.eval()\n",
        "    uncertainties = []\n",
        "    with torch.no_grad():\n",
        "        for images, _ in data_loader:\n",
        "            images = images.to(device)\n",
        "            outputs = model(images)\n",
        "            probs = F.softmax(outputs, dim=1)\n",
        "            uncertainties.extend(1 - torch.max(probs, dim=1)[0].cpu().numpy())\n",
        "    return np.argsort(uncertainties)[::-1]\n",
        "\n",
        "def prediction_entropy(model, data_loader, device):\n",
        "    model.eval()\n",
        "    entropies = []\n",
        "    with torch.no_grad():\n",
        "        for images, _ in data_loader:\n",
        "            images = images.to(device)\n",
        "            outputs = model(images)\n",
        "            probs = F.softmax(outputs, dim=1)\n",
        "            entropy = -torch.sum(probs * torch.log(probs + 1e-6), dim=1).cpu().numpy()\n",
        "            entropies.extend(entropy)\n",
        "    return np.argsort(entropies)[::-1]\n",
        "\n",
        "def margin_sampling(model, data_loader, device):\n",
        "    model.eval()\n",
        "    margins = []\n",
        "    with torch.no_grad():\n",
        "        for images, _ in data_loader:\n",
        "            images = images.to(device)\n",
        "            outputs = model(images)\n",
        "            probs = F.softmax(outputs, dim=1)\n",
        "            sorted_probs, _ = torch.sort(probs, descending=True)\n",
        "            margin = sorted_probs[:, 0] - sorted_probs[:, 1]\n",
        "            margins.extend(margin.cpu().numpy())\n",
        "    return np.argsort(margins)[::-1]\n",
        "\n",
        "def cosine_similarity_selection(model, data_loader, device, labeled_data):\n",
        "    model.eval()\n",
        "    labeled_embeddings = []\n",
        "    with torch.no_grad():\n",
        "        for images, _ in labeled_data:\n",
        "            images = images.to(device)\n",
        "            outputs = model(images)\n",
        "            labeled_embeddings.append(outputs.cpu().numpy())\n",
        "    labeled_embeddings = np.concatenate(labeled_embeddings, axis=0)\n",
        "\n",
        "    similarities = []\n",
        "    with torch.no_grad():\n",
        "        for images, _ in data_loader:\n",
        "            images = images.to(device)\n",
        "            outputs = model(images)\n",
        "            embeddings = outputs.cpu().numpy()\n",
        "            sim = cosine_similarity(embeddings, labeled_embeddings)\n",
        "            similarities.extend(np.max(sim, axis=1))\n",
        "    return np.argsort(similarities)[::-1]\n",
        "\n",
        "def l2_norm_selection(model, data_loader, device, labeled_data):\n",
        "    model.eval()\n",
        "    labeled_embeddings = []\n",
        "    with torch.no_grad():\n",
        "        for images, _ in labeled_data:\n",
        "            images = images.to(device)\n",
        "            outputs = model(images)\n",
        "            labeled_embeddings.append(outputs.cpu().numpy())\n",
        "    labeled_embeddings = np.concatenate(labeled_embeddings, axis=0)\n",
        "\n",
        "    distances = []\n",
        "    with torch.no_grad():\n",
        "        for images, _ in data_loader:\n",
        "            images = images.to(device)\n",
        "            outputs = model(images)\n",
        "            embeddings = outputs.cpu().numpy()\n",
        "            dist = cdist(embeddings, labeled_embeddings, 'euclidean')\n",
        "            distances.extend(np.min(dist, axis=1))\n",
        "    return np.argsort(distances)\n",
        "\n",
        "def kl_divergence_selection(model, data_loader, device):\n",
        "    model.eval()\n",
        "    divergences = []\n",
        "    with torch.no_grad():\n",
        "        for images, _ in data_loader:\n",
        "            images = images.to(device)\n",
        "            outputs = model(images)\n",
        "            probs = F.softmax(outputs, dim=1)\n",
        "            log_probs = F.log_softmax(outputs, dim=1)\n",
        "            divergence = torch.sum(probs * (torch.log(probs + 1e-6) - log_probs), dim=1).cpu().numpy()\n",
        "            divergences.extend(divergence)\n",
        "    return np.argsort(divergences)[::-1]\n",
        "\n",
        "# Initialize datasets\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
        "test_dataset = datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n",
        "\n",
        "# Initialize labeled and unlabeled datasets\n",
        "initial_labeled_samples = 100\n",
        "batch_size = 64\n",
        "\n",
        "labeled_indices = np.random.choice(len(train_dataset), size=initial_labeled_samples, replace=False)\n",
        "unlabeled_indices = np.setdiff1d(np.arange(len(train_dataset)), labeled_indices)\n",
        "\n",
        "labeled_dataset = Subset(train_dataset, labeled_indices)\n",
        "unlabeled_dataset = Subset(train_dataset, unlabeled_indices)\n",
        "\n",
        "labeled_loader = DataLoader(labeled_dataset, batch_size=batch_size, shuffle=True)\n",
        "unlabeled_loader = DataLoader(unlabeled_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Define models\n",
        "class CustomCNN(nn.Module):\n",
        "    def __init__(self, num_conv_layers=2, filter_size=16):\n",
        "        super(CustomCNN, self).__init__()\n",
        "        self.conv_layers = nn.Sequential(\n",
        "            nn.Conv2d(1, filter_size, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2)\n",
        "        )\n",
        "        for _ in range(num_conv_layers - 1):\n",
        "            self.conv_layers.add_module(\n",
        "                f\"conv{_ + 2}\",\n",
        "                nn.Sequential(\n",
        "                    nn.Conv2d(filter_size, filter_size, kernel_size=3, padding=1),\n",
        "                    nn.ReLU(),\n",
        "                    nn.MaxPool2d(2)\n",
        "                )\n",
        "            )\n",
        "        self.fc = nn.Linear(filter_size * 7 * 7, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_layers(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "custom_model = CustomCNN(num_conv_layers=2, filter_size=16).to(device)\n",
        "pretrained_model = models.resnet18(weights=\"IMAGENET1K_V1\")\n",
        "pretrained_model.fc = nn.Linear(pretrained_model.fc.in_features, 10)\n",
        "pretrained_model = pretrained_model.to(device)\n",
        "\n",
        "# Define loss and optimizers\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "custom_optimizer = optim.Adam(custom_model.parameters(), lr=1e-3)\n",
        "pretrained_optimizer = optim.Adam(pretrained_model.parameters(), lr=1e-3)\n",
        "\n",
        "# Train and test functions\n",
        "def train_model(model, data_loader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for images, labels in data_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "    return running_loss / len(data_loader)\n",
        "\n",
        "def test_model(model, data_loader, device):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in data_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    return 100 * correct / total\n",
        "\n",
        "# Active learning loop\n",
        "num_iterations = 5\n",
        "samples_per_iteration = 1000\n",
        "strategy_performance = {\n",
        "    'Least Confidence': [],\n",
        "    'Prediction Entropy': [],\n",
        "    'Margin Sampling': [],\n",
        "    'Cosine Similarity': [],\n",
        "    'L2 Norm': [],\n",
        "    'KL Divergence': []\n",
        "}\n",
        "\n",
        "for iteration in range(num_iterations):\n",
        "    print(f\"\\nActive Learning Iteration {iteration + 1}/{num_iterations}\")\n",
        "\n",
        "    # Train custom model\n",
        "    for epoch in range(5):\n",
        "        train_loss = train_model(custom_model, labeled_loader, criterion, custom_optimizer, device)\n",
        "        print(f\"Custom Model - Iteration {iteration + 1}, Epoch {epoch + 1}, Loss: {train_loss:.4f}\")\n",
        "\n",
        "    custom_accuracy = test_model(custom_model, test_loader, device)\n",
        "    print(f\"Custom Model Test Accuracy after Iteration {iteration + 1}: {custom_accuracy:.2f}%\")\n",
        "\n",
        "    # Train pretrained model\n",
        "    for epoch in range(5):\n",
        "        train_loss = train_model(pretrained_model, labeled_loader, criterion, pretrained_optimizer, device)\n",
        "        print(f\"Pretrained Model - Iteration {iteration + 1}, Epoch {epoch + 1}, Loss: {train_loss:.4f}\")\n",
        "\n",
        "    pretrained_accuracy = test_model(pretrained_model, test_loader, device)\n",
        "    print(f\"Pretrained Model Test Accuracy after Iteration {iteration + 1}: {pretrained_accuracy:.2f}%\")\n",
        "\n",
        "    # Active learning strategies\n",
        "    if len(unlabeled_dataset) > 0:\n",
        "        strategy_results = {}\n",
        "        strategies = [\n",
        "            ('Least Confidence', least_confidence),\n",
        "            ('Prediction Entropy', prediction_entropy),\n",
        "            ('Margin Sampling', margin_sampling),\n",
        "            ('Cosine Similarity', cosine_similarity_selection),\n",
        "            ('L2 Norm', l2_norm_selection),\n",
        "            ('KL Divergence', kl_divergence_selection)\n",
        "        ]\n",
        "\n",
        "        for strategy_name, strategy_fn in strategies:\n",
        "            print(f\"Applying {strategy_name}...\")\n",
        "\n",
        "            selected_indices = strategy_fn(custom_model, unlabeled_loader, device)\n",
        "            selected_indices = selected_indices[:samples_per_iteration]\n",
        "\n",
        "            labeled_indices = np.append(labeled_indices, unlabeled_indices[selected_indices])\n",
        "            unlabeled_indices = np.setdiff1d(unlabeled_indices, unlabeled_indices[selected_indices])\n",
        "\n",
        "            labeled_dataset = Subset(train_dataset, labeled_indices)\n",
        "            unlabeled_dataset = Subset(train_dataset, unlabeled_indices)\n",
        "            labeled_loader = DataLoader(labeled_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "            accuracy = test_model(custom_model, test_loader, device)\n",
        "            strategy_results[strategy_name] = accuracy\n",
        "            print(f\"{strategy_name} Test Accuracy: {accuracy:.2f}%\")\n",
        "\n",
        "        for strategy_name, accuracy in strategy_results.items():\n",
        "            strategy_performance[strategy_name].append(accuracy)\n",
        "\n",
        "average_accuracy_per_strategy\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "CoB80hcgC_xA",
        "outputId": "4f160147-45f7-41ba-fb5a-872e87892e6a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 11.6MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 352kB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 3.20MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 4.22MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "\n",
            "Active Learning Iteration 1/5\n",
            "Custom Model - Iteration 1, Epoch 1, Loss: 2.3071\n",
            "Custom Model - Iteration 1, Epoch 2, Loss: 2.2330\n",
            "Custom Model - Iteration 1, Epoch 3, Loss: 2.1865\n",
            "Custom Model - Iteration 1, Epoch 4, Loss: 2.1203\n",
            "Custom Model - Iteration 1, Epoch 5, Loss: 2.0754\n",
            "Custom Model Test Accuracy after Iteration 1: 16.07%\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Given groups=1, weight of size [64, 3, 7, 7], expected input[64, 1, 28, 28] to have 3 channels, but got 1 channels instead",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-d38a59c66850>\u001b[0m in \u001b[0;36m<cell line: 200>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[0;31m# Train pretrained model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabeled_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpretrained_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Pretrained Model - Iteration {iteration + 1}, Epoch {epoch + 1}, Loss: {train_loss:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-d38a59c66850>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, data_loader, criterion, optimizer, device)\u001b[0m\n\u001b[1;32m    166\u001b[0m         \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36m_forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    266\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_forward_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0;31m# See note [TorchScript super()]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 554\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    555\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    547\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m             )\n\u001b[0;32m--> 549\u001b[0;31m         return F.conv2d(\n\u001b[0m\u001b[1;32m    550\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m         )\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [64, 3, 7, 7], expected input[64, 1, 28, 28] to have 3 channels, but got 1 channels instead"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Define CNN model class\n",
        "def create_model(num_conv_layers, filter_size):\n",
        "    layers = []\n",
        "    in_channels = 1  # Fashion-MNIST has 1 channel\n",
        "\n",
        "    for i in range(num_conv_layers):\n",
        "        out_channels = filter_size * (2 ** i)\n",
        "        layers.append(nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1))\n",
        "        layers.append(nn.ReLU())\n",
        "        layers.append(nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "        in_channels = out_channels\n",
        "\n",
        "    layers.append(nn.Flatten())\n",
        "\n",
        "    # Dynamically compute the input size for the first Linear layer\n",
        "    dummy_input = torch.zeros(1, 1, 28, 28)  # Assuming input size 28x28\n",
        "    with torch.no_grad():\n",
        "        for layer in layers:\n",
        "            dummy_input = layer(dummy_input) if isinstance(layer, (nn.Conv2d, nn.MaxPool2d)) else dummy_input\n",
        "    flatten_size = dummy_input.numel()\n",
        "\n",
        "    layers.append(nn.Linear(flatten_size, 128))\n",
        "    layers.append(nn.ReLU())\n",
        "    layers.append(nn.Linear(128, 10))\n",
        "\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "# Train function\n",
        "def train_model(model, train_loader, criterion, optimizer):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "    return running_loss / len(train_loader)\n",
        "\n",
        "# Test function\n",
        "def test_model(model, test_loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    return 100 * correct / total\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "train_dataset = datasets.FashionMNIST(root=\"./data\", train=True, transform=transform, download=True)\n",
        "test_dataset = datasets.FashionMNIST(root=\"./data\", train=False, transform=transform, download=True)\n",
        "\n",
        "# Hyperparameter combinations\n",
        "configs = [\n",
        "    # {\"num_conv_layers\": 2, \"filter_size\": 16, \"learning_rate\": 0.001, \"batch_size\": 32},\n",
        "    # {\"num_conv_layers\": 2, \"filter_size\": 32, \"learning_rate\": 0.001, \"batch_size\": 64},\n",
        "    # {\"num_conv_layers\": 3, \"filter_size\": 16, \"learning_rate\": 0.001, \"batch_size\": 32},\n",
        "    # {\"num_conv_layers\": 3, \"filter_size\": 32, \"learning_rate\": 0.001, \"batch_size\": 64},\n",
        "    # {\"num_conv_layers\": 2, \"filter_size\": 16, \"learning_rate\": 0.0005, \"batch_size\": 32},\n",
        "    # {\"num_conv_layers\": 2, \"filter_size\": 32, \"learning_rate\": 0.0005, \"batch_size\": 64},\n",
        "    # {\"num_conv_layers\": 3, \"filter_size\": 16, \"learning_rate\": 0.0005, \"batch_size\": 32},\n",
        "    # {\"num_conv_layers\": 3, \"filter_size\": 32, \"learning_rate\": 0.0005, \"batch_size\": 64},\n",
        "    # {\"num_conv_layers\": 4, \"filter_size\": 16, \"learning_rate\": 0.001, \"batch_size\": 32},\n",
        "    {\"num_conv_layers\": 4, \"filter_size\": 32, \"learning_rate\": 0.001, \"batch_size\": 64},\n",
        "]\n",
        "\n",
        "# Results storage\n",
        "results = []\n",
        "\n",
        "# Experiment with configurations\n",
        "for idx, config in enumerate(configs):\n",
        "    print(f\"Running configuration {idx + 1}/{len(configs)}: {config}\")\n",
        "\n",
        "    # Create data loaders\n",
        "    train_loader = DataLoader(train_dataset, batch_size=config[\"batch_size\"], shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=config[\"batch_size\"], shuffle=False)\n",
        "\n",
        "    # Create model\n",
        "    model = create_model(config[\"num_conv_layers\"], config[\"filter_size\"]).to(device)\n",
        "\n",
        "    # Define loss function and optimizer\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=config[\"learning_rate\"])\n",
        "\n",
        "    # Train the model\n",
        "    for epoch in range(5):  # Train for 5 epochs per configuration\n",
        "        train_loss = train_model(model, train_loader, criterion, optimizer)\n",
        "        print(f\"Epoch [{epoch + 1}/5], Loss: {train_loss:.4f}\")\n",
        "\n",
        "    # Test the model\n",
        "    accuracy = test_model(model, test_loader)\n",
        "    print(f\"Accuracy: {accuracy:.2f}%\\n\")\n",
        "\n",
        "    # Store results\n",
        "    results.append({\"config\": config, \"accuracy\": accuracy})\n",
        "\n",
        "# Print summary of results\n",
        "print(\"Experiment Results:\")\n",
        "for result in results:\n",
        "    print(f\"Config: {result['config']}, Accuracy: {result['accuracy']:.2f}%\")\n",
        "\n",
        "\n",
        "# Config: {'num_conv_layers': 4, 'filter_size': 32, 'learning_rate': 0.001, 'batch_size': 64}\n",
        "# best accuracy: Accuracy: 91.11%\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zGiiKNNkEMSm",
        "outputId": "c0553900-604f-44d4-a189-756ea8d090c2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running configuration 1/1: {'num_conv_layers': 4, 'filter_size': 32, 'learning_rate': 0.001, 'batch_size': 64}\n",
            "Epoch [1/5], Loss: 0.5029\n",
            "Epoch [2/5], Loss: 0.3019\n",
            "Epoch [3/5], Loss: 0.2496\n",
            "Epoch [4/5], Loss: 0.2153\n",
            "Epoch [5/5], Loss: 0.1895\n",
            "Accuracy: 91.11%\n",
            "\n",
            "Experiment Results:\n",
            "Config: {'num_conv_layers': 4, 'filter_size': 32, 'learning_rate': 0.001, 'batch_size': 64}, Accuracy: 91.11%\n"
          ]
        }
      ]
    }
  ]
}