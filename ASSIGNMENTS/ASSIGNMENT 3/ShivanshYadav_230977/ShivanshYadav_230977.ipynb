{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "6dc154e1",
      "metadata": {},
      "source": [
        "# Assignment 3\n",
        "Enhance the efficiency and performance of the image classification CNN model from assignment 2 using Active Learning Stratergies"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "69bfa220",
      "metadata": {},
      "source": [
        "# First: Using Neural Networks\n",
        "This is a neural network with two convulational layers with relu activation and SGD optimizer.<br>\n",
        "We also calculate least confidence, prediction entropy, margin sampling, cosine similarity, l2 norm and kl divergence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "cfc823be",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cfc823be",
        "outputId": "83fd5b7a-b21c-4077-83be-fbebe37c9c61"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10, Loss: 0.2380423673719819\n",
            "Epoch 2/10, Loss: 0.05572305065324529\n",
            "Epoch 3/10, Loss: 0.040640152702296796\n",
            "Epoch 4/10, Loss: 0.03136932350921372\n",
            "Epoch 5/10, Loss: 0.025474647406290227\n",
            "Epoch 6/10, Loss: 0.02150734419139197\n",
            "Epoch 7/10, Loss: 0.018364312424345882\n",
            "Epoch 8/10, Loss: 0.015374035355039754\n",
            "Epoch 9/10, Loss: 0.013457818132385703\n",
            "Epoch 10/10, Loss: 0.012166778185696154\n",
            "Accuracy on the test set: 99.23%\n",
            "Average Least Confidence: 0.004795989952981472\n",
            "Average Prediction Entropy: 0.01333171222358942\n",
            "Average Margin Sampling: 0.0047930460423231125\n",
            "Average Cosine Similarity: 0.5263910889625549\n",
            "Average L2 Norm: 0.8339491486549377\n",
            "Average KL Divergence: 1.5235717296600342\n"
          ]
        }
      ],
      "source": [
        "# Importing necessary libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from sklearn.metrics import pairwise_distances\n",
        "import numpy as np\n",
        "\n",
        "# Define the transform for MNIST dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))  # For grayscale images\n",
        "])\n",
        "\n",
        "# Download and load the MNIST dataset\n",
        "trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=4, shuffle=False, num_workers=2)\n",
        "\n",
        "# Define the classes for MNIST dataset (digits 0-9)\n",
        "classes = ('0', '1', '2', '3', '4', '5', '6', '7', '8', '9')\n",
        "\n",
        "# Define the neural network architecture for MNIST\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        # Convolutional layers\n",
        "        self.conv1 = nn.Conv2d(1, 6, 5)  # Input channels: 1 (grayscale), Output channels: 6, Kernel size: 5x5\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)  # Input channels: 6, Output channels: 16, Kernel size: 5x5\n",
        "        # Fully connected layers\n",
        "        # Adjust input size after convolution (28x28 -> 12x12 -> 4x4)\n",
        "        self.fc1 = nn.Linear(16 * 4 * 4, 120)  # Adjusted for MNIST image size\n",
        "        self.fc2 = nn.Linear(120, 84)  # Input features: 120, Output features: 84\n",
        "        self.fc3 = nn.Linear(84, 10)  # Output features: 10 (digits 0-9)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.max_pool2d(F.relu(self.conv1(x)), 2, 2)  # First convolution and max pooling\n",
        "        x = F.max_pool2d(F.relu(self.conv2(x)), 2, 2)  # Second convolution and max pooling\n",
        "        x = x.view(-1, 16 * 4 * 4)  # Flatten the output before passing to fully connected layers\n",
        "        x = F.relu(self.fc1(x))  # First fully connected layer\n",
        "        x = F.relu(self.fc2(x))  # Second fully connected layer\n",
        "        x = self.fc3(x)  # Final output layer\n",
        "        return x\n",
        "\n",
        "# Instantiate the network\n",
        "net = Net()\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()  # Loss function for multi-class classification\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)  # SGD optimizer with momentum\n",
        "epochs = 10  # Number of epochs\n",
        "\n",
        "\n",
        "# Creating a function to calculate uncertainty for each epoch metrics\n",
        "def calculate_uncertainty_metrics(outputs):\n",
        "    # Convert NumPy array to PyTorch tensor\n",
        "    outputs_tensor = torch.from_numpy(outputs)\n",
        "    # Applying softmax along dimension 1\n",
        "    probabilities = F.softmax(outputs_tensor, dim=1)\n",
        "    # Least Confidence: 1 - Maximum probability for each sample\n",
        "    least_confidence = 1 - probabilities.max(dim=1).values.cpu().detach().numpy()\n",
        "    # Handling NaN in prediction entropy\n",
        "    current_probs = probabilities.clone().detach()\n",
        "    current_probs[current_probs == 0] = 1e-10  # Adding a small epsilon to avoid log(0)\n",
        "    # Prediction Entropy: Negative sum of (probability * log(probability)) for each class\n",
        "    prediction_entropy = -torch.sum(current_probs * torch.log(current_probs), dim=1).cpu().detach().numpy()\n",
        "    # Margin Sampling: 1 - (Maximum probability - Minimum probability) for each sample\n",
        "    margin_sampling = 1 - torch.max(probabilities, dim=1).values.cpu().detach().numpy() - \\\n",
        "                      torch.min(probabilities, dim=1).values.cpu().detach().numpy()\n",
        "    # Returning least confidence,prediction entropy and margin sampling values obtained\n",
        "    return least_confidence, prediction_entropy, margin_sampling\n",
        "\n",
        "# Creating a function to caalculate diversiy metrics\n",
        "def calculate_diversity_metrics(features, m=5):\n",
        "    # Calculate pairwise distances using cosine similarity\n",
        "    feature_distances = pairwise_distances(features.cpu().detach().numpy(), metric='cosine')\n",
        "    # Cosine Similarity: 1 - Mean cosine similarity with the top m neighbors for each sample\n",
        "    cosine_similarity = 1 - feature_distances[:, 1:m+1].mean(axis=1)\n",
        "    # Calculate pairwise distances using L2 (Euclidean) norm\n",
        "    l2_distances = pairwise_distances(features.cpu().detach().numpy(), metric='euclidean')\n",
        "    # L2 Norm: Mean L2 norm with the top m neighbors for each sample\n",
        "    l2_norm = l2_distances[:, 1:m+1].mean(axis=1)\n",
        "    # Returning cosine similarity and l2 norm values obtained\n",
        "    return cosine_similarity, l2_norm\n",
        "\n",
        "# Creating a functiom to calculate kl divergence\n",
        "def calculate_kl_divergence(outputs, feature_distances, m=5):\n",
        "    # List to store KL divergence scores for each sample\n",
        "    kl_divergence = []\n",
        "    # Iterate over each sample in the outputs\n",
        "    for i in range(len(outputs)):\n",
        "        # Calculate the probability distribution of the current sample\n",
        "        current_sample_prob = F.softmax(outputs[i], dim=0)\n",
        "        # Get the indices of the top m neighbors for the current sample\n",
        "        neighbor_indices = feature_distances[i, 1:m+1].astype(int)\n",
        "        # Calculate the average probability distribution of the neighbors\n",
        "        neighbors_prob = torch.mean(F.softmax(outputs[neighbor_indices], dim=1), dim=0)\n",
        "        # Calculate KL divergence between the current sample and its neighbors\n",
        "        # Include 'reduction' argument inside F.kl_div\n",
        "        kl_divergence.append(F.kl_div(torch.log(current_sample_prob), neighbors_prob, reduction='batchmean'))\n",
        "\n",
        "    # Returning kl divergence values obtained\n",
        "    return kl_divergence\n",
        "\n",
        "# Creating a Function to calculate uncertainty and diversity metrics\n",
        "def calculate_metrics(outputs, features, m=5):\n",
        "    # Calculate uncertainty metrics\n",
        "    least_confidence, prediction_entropy, margin_sampling = calculate_uncertainty_metrics(outputs.detach().numpy())\n",
        "    # Extend lists with uncertainty metrics\n",
        "    least_confidence_list.extend(torch.from_numpy(least_confidence))\n",
        "    prediction_entropy_list.extend(torch.from_numpy(prediction_entropy))\n",
        "    margin_sampling_list.extend(torch.from_numpy(margin_sampling))\n",
        "    # Calculate diversity metrics\n",
        "    features_normalized = F.normalize(features, p=2, dim=1)\n",
        "    cosine_similarity, l2_norm = calculate_diversity_metrics(features_normalized)\n",
        "    # cosine_similarity, l2_norm = calculate_diversity_metrics(features)\n",
        "    # Extend lists with diversity metrics\n",
        "    cosine_similarity_list.extend(torch.from_numpy(cosine_similarity))\n",
        "    l2_norm_list.extend(torch.from_numpy(l2_norm))\n",
        "    feature_distances = pairwise_distances(features.cpu().detach().numpy(), metric='cosine')\n",
        "    # Calculate KL divergence scores\n",
        "    kl_divergence_scores = calculate_kl_divergence(outputs, feature_distances, m=5)\n",
        "    # Extend the list with KL divergence scores\n",
        "    kl_divergence_list.extend(kl_divergence_scores)\n",
        "\n",
        "# Define lists for uncertainty and diversity metrics\n",
        "least_confidence_list = []\n",
        "prediction_entropy_list = []\n",
        "margin_sampling_list = []\n",
        "cosine_similarity_list = []\n",
        "l2_norm_list = []\n",
        "kl_divergence_list = []\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        inputs, labels = data\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss / len(trainloader)}\")\n",
        "\n",
        "# # Testing the model\n",
        "# correct = 0\n",
        "# total = 0\n",
        "\n",
        "# # Use torch.no_grad() to disable gradient computation during testing\n",
        "# with torch.no_grad():\n",
        "#     for data in testloader:\n",
        "#         images, labels = data\n",
        "#         outputs = net(images)\n",
        "#         _, predicted = torch.max(outputs.data, 1)\n",
        "#         total += labels.size(0)\n",
        "#         correct += (predicted == labels).sum().item()\n",
        "\n",
        "#         # Extract features using the first convolutional layer\n",
        "#         features = net.conv1(images)\n",
        "#         features = F.max_pool2d(F.relu(features), 2, 2)\n",
        "#         features = net.conv2(features)\n",
        "#         features = F.max_pool2d(F.relu(features), 2, 2)\n",
        "#         features = features.view(features.size(0), -1)\n",
        "\n",
        "#         # Here, you should calculate uncertainty and diversity metrics as needed\n",
        "#         # calculate_metrics(outputs, features)\n",
        "\n",
        "# accuracy = 100 * correct / total\n",
        "# print(f\"Accuracy on the test set: {accuracy:.2f}%\")\n",
        "\n",
        "\n",
        "# Testing the model\n",
        "correct = 0  # Initialize the number of correctly predicted samples\n",
        "total = 0  # Initialize the total number of samples\n",
        "\n",
        "# Initialize lists to store uncertainty and diversity measures\n",
        "least_confidence_list = [] # Initialize lists to store least confidence measures\n",
        "prediction_entropy_list = [] # Initialize lists to store prediction entropy measures\n",
        "margin_sampling_list = [] # Initialize lists to store margin sampling measures\n",
        "cosine_similarity_list = [] # Initialize lists to store cosine similarity measures\n",
        "l2_norm_list = [] # Initialize lists to store L2 norm estimates\n",
        "kl_divergence_list = [] # Initialize lists to store KL divergence estimates\n",
        "\n",
        "# Use torch.no_grad() to disable gradient computation during testing\n",
        "with torch.no_grad():\n",
        "    for data in testloader:  # Iterate over batches in the test loader\n",
        "        images, labels = data  # Get inputs and true labels for the current batch\n",
        "        outputs = net(images)  # Forward pass to compute predicted outputs\n",
        "        _, predicted = torch.max(outputs.data, 1)  # Get the index of the maximum predicted value\n",
        "        total += labels.size(0)  # Increment the total number of samples by the batch size\n",
        "        correct += (predicted == labels).sum().item()  # Count the number of correctly predicted samples\n",
        "\n",
        "        # Extract features using the first convolutional layer\n",
        "        features = net.conv1(images)\n",
        "        # Apply max pooling and ReLU activation\n",
        "        features = F.max_pool2d(F.relu(features), 2, 2)\n",
        "        # Extract features using the second convolutional layer\n",
        "        features = net.conv2(features)\n",
        "        # Apply max pooling and ReLU activation\n",
        "        features = F.max_pool2d(F.relu(features), 2, 2)\n",
        "        # Flatten the features to be used in fully connected layers\n",
        "        features = features.view(features.size(0), -1)\n",
        "        # Calculate metrics using the extracted features and model outputs\n",
        "        calculate_metrics(outputs, features)\n",
        "\n",
        "accuracy = 100 * correct / total # Calculate accuracy\n",
        "print(f\"Accuracy on the test set: {accuracy:.2f}%\") # Print the accuracy on test set\n",
        "# Print the average values of uncertainty and diversity measures\n",
        "print(f\"Average Least Confidence: {torch.mean(torch.stack(least_confidence_list))}\") # Print the average value of least confidence\n",
        "print(f\"Average Prediction Entropy: {torch.mean(torch.stack(prediction_entropy_list))}\") # Print the average value of prediction entropy\n",
        "print(f\"Average Margin Sampling: {torch.mean(torch.stack(margin_sampling_list))}\") # Print the average value of margin sampling\n",
        "print(f\"Average Cosine Similarity: {torch.mean(torch.stack(cosine_similarity_list))}\") # Print the average value of cosine similarity\n",
        "print(f\"Average L2 Norm: {torch.mean(torch.stack(l2_norm_list))}\") # Print the average value of L2 norm\n",
        "print(f\"Average KL Divergence: {torch.mean(torch.stack(kl_divergence_list))}\") # Print the average value of KL divergence\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d0bd1b7",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "1509e7ee",
      "metadata": {},
      "source": [
        "# Second: Using CNN Architecture\n",
        "(same as used in assignment 2)\n",
        "This is a CNN model with two convulational layers with Adam optimizer and maxpool and using cuda for encorporating GPU for the computation.<br>\n",
        "We also calculate least confidence, prediction entropy, margin sampling, cosine similarity, l2 norm and kl divergence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1db7d24",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f1db7d24",
        "outputId": "8df5b3b8-25a6-4fd8-af70-c2d22a30d129"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10, Loss: 0.013133395463228226\n",
            "Epoch 2/10, Loss: 0.11068765074014664\n",
            "Epoch 3/10, Loss: 0.011432253755629063\n",
            "Epoch 4/10, Loss: 0.010805139318108559\n",
            "Epoch 5/10, Loss: 0.0041971271857619286\n",
            "Epoch 6/10, Loss: 0.026356974616646767\n",
            "Epoch 7/10, Loss: 0.005212348885834217\n",
            "Epoch 8/10, Loss: 2.0116024188610027e-06\n",
            "Epoch 9/10, Loss: 0.00017639149155002087\n",
            "Epoch 10/10, Loss: 6.183942673487763e-07\n",
            "Test Accuracy: 98.80%\n",
            "Average Least Confidence: 0.0040636989288032055\n",
            "Average Prediction Entropy: 0.010589069686830044\n",
            "Average Margin Sampling: 0.0040637110359966755\n",
            "Average Cosine Similarity: 0.7735907435417175\n",
            "Average L2 Norm: 0.6562566161155701\n",
            "Average KL Divergence: 1.9077818393707275\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn.functional as F\n",
        "from sklearn.metrics import pairwise_distances\n",
        "from scipy.stats import entropy\n",
        "\n",
        "# Define the CNN architecture by creating a class that inherits from nn.Module\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "\n",
        "        # Adjusted the convolutional layers for MNIST (grayscale 1 channel, 28x28 image size)\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)  # Input: 1 channel (grayscale), Output: 32 filters\n",
        "        self.relu = nn.ReLU()\n",
        "        self.maxpool = nn.MaxPool2d(2)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)  # Input: 32 channels, Output: 64 filters\n",
        "\n",
        "        self.flatten = nn.Flatten()\n",
        "\n",
        "        # Fully connected layers (adjusted based on output of conv layers)\n",
        "        self.fc1 = nn.Linear(64 * 7 * 7, 512)  # Adjust input size based on feature map size\n",
        "        self.fc2 = nn.Linear(512, 10)  # Output: 10 neurons (for 10 classes in MNIST)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.conv2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.flatten(x)\n",
        "\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "# Set the device for computations (GPU if available, otherwise CPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Define hyperparameters\n",
        "batch_size = 32\n",
        "learning_rate = 0.001\n",
        "epochs = 10\n",
        "m = 5\n",
        "\n",
        "# Define data transformations for preprocessing\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))  # Adjusted for MNIST (1 channel)\n",
        "])\n",
        "\n",
        "# Download and load the MNIST dataset\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Initialize the neural network model, loss function, and optimizer\n",
        "model = SimpleCNN().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Define the uncertainty and diversity metrics functions (unchanged)\n",
        "\n",
        "# Train the model\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item()}\")\n",
        "\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "# Creating a Function to calculate uncertainty and diversity metrics\n",
        "def calculate_metrics(outputs, features, m=5):\n",
        "    # Calculate uncertainty metrics\n",
        "    least_confidence, prediction_entropy, margin_sampling = calculate_uncertainty_metrics(outputs.detach().numpy())\n",
        "    # Extend lists with uncertainty metrics\n",
        "    least_confidence_list.extend(torch.from_numpy(least_confidence))\n",
        "    prediction_entropy_list.extend(torch.from_numpy(prediction_entropy))\n",
        "    margin_sampling_list.extend(torch.from_numpy(margin_sampling))\n",
        "    # Calculate diversity metrics\n",
        "    features_normalized = F.normalize(features, p=2, dim=1)\n",
        "    cosine_similarity, l2_norm = calculate_diversity_metrics(features_normalized)\n",
        "    # cosine_similarity, l2_norm = calculate_diversity_metrics(features)\n",
        "    # Extend lists with diversity metrics\n",
        "    cosine_similarity_list.extend(torch.from_numpy(cosine_similarity))\n",
        "    l2_norm_list.extend(torch.from_numpy(l2_norm))\n",
        "    feature_distances = pairwise_distances(features.cpu().detach().numpy(), metric='cosine')\n",
        "    # Calculate KL divergence scores\n",
        "    kl_divergence_scores = calculate_kl_divergence(outputs, feature_distances, m=5)\n",
        "    # Extend the list with KL divergence scores\n",
        "    kl_divergence_list.extend(kl_divergence_scores)\n",
        "\n",
        "\n",
        "def calculate_metrics(outputs, features, m=5):\n",
        "    # Ensure the tensor is on the CPU before converting to NumPy\n",
        "    outputs_cpu = outputs.detach().cpu()  # Move tensor to CPU and detach it from computation graph\n",
        "\n",
        "    # Now call the uncertainty metrics function with the CPU tensor\n",
        "    least_confidence, prediction_entropy, margin_sampling = calculate_uncertainty_metrics(outputs_cpu)\n",
        "\n",
        "    # Extend lists with uncertainty metrics\n",
        "    least_confidence_list.extend(torch.from_numpy(least_confidence))\n",
        "    prediction_entropy_list.extend(torch.from_numpy(prediction_entropy))\n",
        "    margin_sampling_list.extend(torch.from_numpy(margin_sampling))\n",
        "\n",
        "    # Calculate diversity metrics\n",
        "    features_normalized = F.normalize(features, p=2, dim=1)\n",
        "    cosine_similarity, l2_norm = calculate_diversity_metrics(features_normalized)\n",
        "\n",
        "    # Extend the list with diversity metrics\n",
        "    cosine_similarity_list.extend(torch.from_numpy(cosine_similarity))\n",
        "    l2_norm_list.extend(torch.from_numpy(l2_norm))\n",
        "\n",
        "    feature_distances = pairwise_distances(features.cpu().detach().numpy(), metric='cosine')\n",
        "\n",
        "    # Calculate KL divergence scores\n",
        "    kl_divergence_scores = calculate_kl_divergence(outputs_cpu, feature_distances, m=5)\n",
        "\n",
        "    # Extend the list with KL divergence scores\n",
        "    kl_divergence_list.extend(kl_divergence_scores)\n",
        "\n",
        "\n",
        "def calculate_uncertainty_metrics(outputs):\n",
        "    # outputs is already a tensor, no need to convert\n",
        "    # Applying softmax along dimension 1\n",
        "    probabilities = F.softmax(outputs, dim=1)\n",
        "    # Least Confidence: 1 - Maximum probability for each sample\n",
        "    least_confidence = 1 - probabilities.max(dim=1).values.cpu().detach().numpy()\n",
        "    # Handling NaN in prediction entropy\n",
        "    current_probs = probabilities.clone().detach()\n",
        "    current_probs[current_probs == 0] = 1e-10  # Adding a small epsilon to avoid log(0)\n",
        "    ## Uses the formula for entropy: H(p)=−∑p(x)log(p(x).\n",
        "    ## Multiplies each probability by its logarithm, sums these values for each sample, and negates the result.\n",
        "    ## Entropy measures the \"spread\" or uncertainty of the predicted probabilities. Higher entropy indicates greater uncertainty.\n",
        "    prediction_entropy = -torch.sum(current_probs * torch.log(current_probs), dim=1).cpu().detach().numpy()\n",
        "    ## Calculates the difference between the maximum and minimum predicted probabilities for each sample.\n",
        "    ## Subtracts this difference from 1 to obtain the margin sampling score.\n",
        "    ## A smaller margin (closer to 1) indicates higher uncertainty, as the model is less confident in discriminating between classes.\n",
        "    # Margin Sampling: 1 - (Maximum probability - Minimum probability) for each sample\n",
        "    margin_sampling = 1 - (torch.max(probabilities, dim=1).values.cpu().detach().numpy() - \\\n",
        "                           torch.min(probabilities, dim=1).values.cpu().detach().numpy())\n",
        "    ## Returns three metrics:\n",
        "    ## least_confidence: A measure of how uncertain the model is about its most likely class.\n",
        "    ## prediction_entropy: The overall uncertainty in the prediction distribution.\n",
        "    ## margin_sampling: A measure of the closeness of the most likely predictions.\n",
        "    return least_confidence, prediction_entropy, margin_sampling\n",
        "\n",
        "# Summary of Logic:\n",
        "# Converts raw model outputs (logits) into probabilities.Calculates three distinct uncertainty metrics to\n",
        "# analyze the model's predictions from different perspectives:\n",
        "# Least Confidence: Focuses on the top prediction.\n",
        "# Entropy: Considers the overall distribution.\n",
        "# Margin Sampling: Examines the spread between top and bottom probabilities.\n",
        "# These metrics can be used to select samples with high uncertainty for further inspection or active learning.\n",
        "\n",
        "# Use Case: This function is particularly useful in:\n",
        "# Active Learning: Selecting uncertain samples to label for model improvement.\n",
        "# Model Evaluation: Analyzing prediction confidence and uncertainty for robust decision-making.\n",
        "# Anomaly Detection: Identifying outliers based on uncertainty metrics.\n",
        "\n",
        "\n",
        "# Define a function to calculate diversity metrics\n",
        "# The function calculates two diversity metrics for a set of feature vectors\n",
        "# Cosine Similarity: Measures the angular similarity between a feature vector and its neighbors.\n",
        "# L2 Norm (Euclidean Distance): Measures the average Euclidean distance between a feature vector and its neighbors.\n",
        "\n",
        "# Input Arguments:\n",
        "# features: A tensor containing feature vectors for all samples, typically obtained from a model's intermediate layer.\n",
        "# m: The number of nearest neighbors to consider for diversity metrics (default value is 5).\n",
        "def calculate_diversity_metrics(features, m=5):\n",
        "\n",
        "    ## Converts the features tensor into a NumPy array for computation.\n",
        "    ## Uses the pairwise_distances function to calculate cosine distances between each pair of feature vectors.\n",
        "    ## Cosine Distance = 1 - Cosine Similarity.\n",
        "    feature_distances = pairwise_distances(features.cpu().detach().numpy(), metric='cosine')  # Cosine distance\n",
        "\n",
        "    ## feature_distances[:, 1:m+1] selects the distances to the nearest m neighbors (ignoring the distance to the sample itself, which is at index 0).\n",
        "    ## Takes the mean of these distances for each feature vector.\n",
        "    ## Since cosine similarity is the complement of cosine distance, it computes the similarity as 1 - distance.\n",
        "    cosine_similarity = 1 - feature_distances[:, 1:m+1].mean(axis=1)  # Cosine similarity\n",
        "\n",
        "    l2_distances = pairwise_distances(features.cpu().detach().numpy(), metric='euclidean')  # Computes the Euclidean (L2) distances between each pair of feature vectors\n",
        "\n",
        "    ## Selects the distances to the nearest m neighbors (excluding the sample itself).\n",
        "    ## Computes the mean of these distances for each feature vector.\n",
        "    l2_norm = l2_distances[:, 1:m+1].mean(axis=1)  # L2 norm\n",
        "\n",
        "    ## Returns two metrics for each feature vector:\n",
        "    ## cosine_similarity: The average cosine similarity to its m nearest neighbors.\n",
        "    ## l2_norm: The average L2 distance to its m nearest neighbors.\n",
        "    return cosine_similarity, l2_norm\n",
        "\n",
        "## Summary of Logic:\n",
        "## For each feature vector:\n",
        "## Find the distances to its m nearest neighbors based on cosine and Euclidean distance metrics.\n",
        "## Compute the average cosine similarity and L2 distance (norm).\n",
        "## The results indicate how diverse the feature vectors are:\n",
        "## High cosine similarity suggests that the feature vectors are closely aligned (less diverse).\n",
        "## Low L2 norm indicates that the feature vectors are tightly clustered in feature space.\n",
        "\n",
        "## Use Case:\n",
        "## This function can be used to analyze the diversity of feature vectors, understand cluster compactness,\n",
        "## and evaluate how well-separated or grouped the features are in the latent space of a model.\n",
        "\n",
        "# Define a function to calculate KL divergence\n",
        "## Input Arguments:\n",
        "## outputs: A tensor containing the model's raw output logits for all samples.\n",
        "## feature_distances: A matrix of pairwise distances between feature vectors for all samples, where feature_distances[i, j] represents\n",
        "## the distance between sample i and j.\n",
        "## m: The number of nearest neighbors to consider for each sample (default is 5).\n",
        "def calculate_kl_divergence(outputs, feature_distances, m=5):\n",
        "    kl_divergence = []  # List to store KL divergence values for each sample.\n",
        "    for i in range(len(outputs)):    ## The loop iterates through each sample i in the dataset.\n",
        "        ## The outputs[i] contains the raw logits for sample i. The softmax function converts logits into a probability\n",
        "        ## distribution  P(i) over the classes.\n",
        "        current_sample_prob = F.softmax(outputs[i], dim=0)  # Softmax for the current sample\n",
        "\n",
        "        ## The row feature_distances[i] contains distances of sample i from all other samples.\n",
        "        ## feature_distances[i, 1:m+1] retrieves the indices of the nearest m neighbors (excluding itself, which is at index 0).\n",
        "        neighbor_indices = feature_distances[i, 1:m+1].astype(int)  # Indices of nearest neighbors\n",
        "\n",
        "        ## outputs[neighbor_indices] retrieves the logits of the m nearest neighbors of sample i.\n",
        "        ## Softmax is applied along the second dimension (dim=1) to convert logits into probabilities for all neighbors.\n",
        "        ## The mean is computed across neighbors to obtain the average probability distribution Q(i).\n",
        "        neighbors_prob = torch.mean(F.softmax(outputs[neighbor_indices], dim=1), dim=0)  # Average neighbor probabilities\n",
        "\n",
        "        ## Small values (1e−10) are added to both probability distributions to avoid issues with zero probabilities during the logarithm computation.\n",
        "        epsilon = 1e-10\n",
        "        current_sample_prob += epsilon\n",
        "        neighbors_prob += epsilon\n",
        "\n",
        "        ## The logarithm of the current sample's probability distribution P(i) is computed using torch.log(current_sample_prob).\n",
        "        ## F.kl_div calculates the KL divergence\n",
        "        ## reduction='batchmean' ensures the result is averaged across all classes.\n",
        "\n",
        "        kl_divergence.append(F.kl_div(torch.log(current_sample_prob), neighbors_prob, reduction='batchmean'))  # KL Divergence\n",
        "    return kl_divergence\n",
        "\n",
        "## Summary of Logic:\n",
        "## For each sample: Compute its probability distribution using softmax. Find the indices of its m nearest neighbors based\n",
        "## on feature distances. Compute the average probability distribution of the neighbors. Calculate the KL divergence between the sample's distribution\n",
        "## and the average distribution of its neighbors. The output is a list of KL divergence values, where each value corresponds to a sample in the dataset.\n",
        "## Use Case:\n",
        "## Outlier Detection: Samples with high KL divergence have significantly different predictions compared to their neighbors, suggesting potential outliers or inconsistent predictions.\n",
        "## Model Uncertainty: High KL divergence indicates regions in the feature space where the model may be less reliable.\n",
        "\n",
        "least_confidence_list = []\n",
        "prediction_entropy_list = []\n",
        "margin_sampling_list = []\n",
        "cosine_similarity_list = []\n",
        "l2_norm_list = []\n",
        "kl_divergence_list = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "        # Extract features from the convolutional layers\n",
        "        conv1_output = model.conv1(images)\n",
        "        relu_output = model.relu(conv1_output)\n",
        "        maxpool_output = model.maxpool(relu_output)\n",
        "        conv2_output = model.conv2(maxpool_output)\n",
        "        features = model.flatten(conv2_output)\n",
        "        features = features.view(features.size(0), -1)\n",
        "\n",
        "        # Calculate metrics (metrics functions not modified)\n",
        "        calculate_metrics(outputs, features)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = correct / total\n",
        "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "# Print the average values of uncertainty and diversity measures\n",
        "print(f\"Average Least Confidence: {torch.mean(torch.stack(least_confidence_list))}\")\n",
        "print(f\"Average Prediction Entropy: {torch.mean(torch.stack(prediction_entropy_list))}\")\n",
        "print(f\"Average Margin Sampling: {torch.mean(torch.stack(margin_sampling_list))}\")\n",
        "print(f\"Average Cosine Similarity: {torch.mean(torch.stack(cosine_similarity_list))}\")\n",
        "print(f\"Average L2 Norm: {torch.mean(torch.stack(l2_norm_list))}\")\n",
        "print(f\"Average KL Divergence: {torch.mean(torch.stack(kl_divergence_list))}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c1b21d23",
      "metadata": {},
      "source": [
        "# Third: Using a Pretrained Model - RESNET18\n",
        "(using this for better accuracy)\n",
        "This is a CNN model incorporating the pretrained model RESNET18, it has one convolutional layer and one fully connected layer<br>\n",
        "RESNET18 takes input in three channels so we have normalized the input for one channel in grayscale.<br>\n",
        "We also calculate least confidence, prediction entropy, margin sampling, cosine similarity, l2 norm and kl divergence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "GJ_50Il6d0kE",
      "metadata": {
        "id": "GJ_50Il6d0kE"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "c:\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10, Loss: 0.12671785056591034\n",
            "Epoch 2/10, Loss: 0.005561455152928829\n",
            "Epoch 3/10, Loss: 0.06107950210571289\n",
            "Epoch 4/10, Loss: 0.12433136999607086\n",
            "Epoch 5/10, Loss: 0.0018317534122616053\n",
            "Epoch 6/10, Loss: 0.03611539304256439\n",
            "Epoch 7/10, Loss: 0.003682696493342519\n",
            "Epoch 8/10, Loss: 0.0002998908457811922\n",
            "Epoch 9/10, Loss: 0.009061828255653381\n",
            "Epoch 10/10, Loss: 0.0004075284523423761\n",
            "Test Accuracy: 99.31%\n",
            "Average Least Confidence: 0.005538662895560265\n",
            "Average Prediction Entropy: 0.01951856166124344\n",
            "Average Margin Sampling: 0.005533408373594284\n",
            "Average Cosine Similarity: 0.3630516231060028\n",
            "Average L2 Norm: 1.075036883354187\n",
            "Average KL Divergence: 1.4472111463546753\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn.functional as F\n",
        "from sklearn.metrics import pairwise_distances\n",
        "from scipy.stats import entropy\n",
        "import torchvision.models as models  # Import pretrained models\n",
        "\n",
        "# Define the pretrained ResNet18 architecture\n",
        "class ResNet18ForMNIST(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(ResNet18ForMNIST, self).__init__()\n",
        "        self.resnet18 = models.resnet18(pretrained=True)\n",
        "        self.resnet18.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
        "        self.resnet18.fc = nn.Linear(self.resnet18.fc.in_features, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Extract features from layer4\n",
        "        x = self.resnet18.conv1(x)\n",
        "        x = self.resnet18.bn1(x)\n",
        "        x = self.resnet18.relu(x)\n",
        "        x = self.resnet18.maxpool(x)\n",
        "\n",
        "        x = self.resnet18.layer1(x)\n",
        "        x = self.resnet18.layer2(x)\n",
        "        x = self.resnet18.layer3(x)\n",
        "        features = self.resnet18.layer4(x)  # Extract features here\n",
        "\n",
        "        x = self.resnet18.avgpool(features)\n",
        "        x = torch.flatten(x, 1)\n",
        "        output = self.resnet18.fc(x)  # Final output\n",
        "        return output, features\n",
        "\n",
        "\n",
        "# Set the device for computations (GPU if available, otherwise CPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Define hyperparameters\n",
        "batch_size = 32\n",
        "learning_rate = 0.001\n",
        "epochs = 10\n",
        "m = 5\n",
        "\n",
        "# Define data transformations for preprocessing\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(), # Remove the transforms.Grayscale to keep the images as 1-channel grayscale\n",
        "    transforms.Normalize((0.5,), (0.5,)) # Normalization for 1-channel images\n",
        "])\n",
        "\n",
        "# Download and load the MNIST dataset\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Initialize the neural network model, loss function, and optimizer\n",
        "model = ResNet18ForMNIST().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Define the uncertainty and diversity metrics functions (unchanged)\n",
        "\n",
        "# Train the model\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # Unpack output and features\n",
        "        outputs, features = model(images)  # `features` is unused during training\n",
        "        \n",
        "        # Compute loss only using `outputs`\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        torch.cuda.empty_cache()\n",
        "    \n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item()}\")\n",
        "\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "# Creating a function to calculate uncertainty for each epoch metrics\n",
        "def calculate_uncertainty_metrics(outputs):\n",
        "    # Convert NumPy array to PyTorch tensor\n",
        "    outputs_tensor = torch.from_numpy(outputs)\n",
        "    # Applying softmax along dimension 1\n",
        "    probabilities = F.softmax(outputs_tensor, dim=1)\n",
        "    # Least Confidence: 1 - Maximum probability for each sample\n",
        "    least_confidence = 1 - probabilities.max(dim=1).values.cpu().detach().numpy()\n",
        "    # Handling NaN in prediction entropy\n",
        "    current_probs = probabilities.clone().detach()\n",
        "    current_probs[current_probs == 0] = 1e-10  # Adding a small epsilon to avoid log(0)\n",
        "    # Prediction Entropy: Negative sum of (probability * log(probability)) for each class\n",
        "    prediction_entropy = -torch.sum(current_probs * torch.log(current_probs), dim=1).cpu().detach().numpy()\n",
        "    # Margin Sampling: 1 - (Maximum probability - Minimum probability) for each sample\n",
        "    margin_sampling = 1 - torch.max(probabilities, dim=1).values.cpu().detach().numpy() - \\\n",
        "                      torch.min(probabilities, dim=1).values.cpu().detach().numpy()\n",
        "    # Returning least confidence,prediction entropy and margin sampling values obtained\n",
        "    return least_confidence, prediction_entropy, margin_sampling\n",
        "\n",
        "# Creating a function to caalculate diversiy metrics\n",
        "def calculate_diversity_metrics(features, m=5):\n",
        "    # Calculate pairwise distances using cosine similarity\n",
        "    feature_distances = pairwise_distances(features.cpu().detach().numpy(), metric='cosine')\n",
        "    # Cosine Similarity: 1 - Mean cosine similarity with the top m neighbors for each sample\n",
        "    cosine_similarity = 1 - feature_distances[:, 1:m+1].mean(axis=1)\n",
        "    # Calculate pairwise distances using L2 (Euclidean) norm\n",
        "    l2_distances = pairwise_distances(features.cpu().detach().numpy(), metric='euclidean')\n",
        "    # L2 Norm: Mean L2 norm with the top m neighbors for each sample\n",
        "    l2_norm = l2_distances[:, 1:m+1].mean(axis=1)\n",
        "    # Returning cosine similarity and l2 norm values obtained\n",
        "    return cosine_similarity, l2_norm\n",
        "\n",
        "# Creating a functiom to calculate kl divergence\n",
        "def calculate_kl_divergence(outputs, feature_distances, m=5):\n",
        "    # List to store KL divergence scores for each sample\n",
        "    kl_divergence = []\n",
        "\n",
        "    # Add a small epsilon to avoid log(0) and ensure numerical stability\n",
        "    epsilon = 1e-10\n",
        "\n",
        "    # Iterate over each sample in the outputs\n",
        "    for i in range(len(outputs)):\n",
        "        # Calculate the probability distribution of the current sample (softmax)\n",
        "        current_sample_prob = F.softmax(outputs[i], dim=0)\n",
        "        \n",
        "        # Add epsilon to avoid log(0) when computing log-probabilities\n",
        "        current_sample_log_prob = torch.log(current_sample_prob + epsilon)\n",
        "\n",
        "        # Get the indices of the top m neighbors for the current sample from the distance matrix\n",
        "        neighbor_indices = feature_distances[i, 1:m+1].astype(int)  # Exclude the first index, which is the sample itself\n",
        "        \n",
        "        # Calculate the average probability distribution of the neighbors\n",
        "        neighbors_prob = torch.mean(F.softmax(outputs[neighbor_indices], dim=1), dim=0)\n",
        "\n",
        "        # Add epsilon to the neighbor probabilities to avoid log(0) in KL divergence\n",
        "        neighbors_prob = neighbors_prob + epsilon\n",
        "\n",
        "        # Calculate KL divergence between the current sample and its neighbors\n",
        "        kl_score = F.kl_div(current_sample_log_prob, neighbors_prob, reduction='batchmean')\n",
        "\n",
        "        # Store the KL divergence for this sample as a Tensor\n",
        "        kl_divergence.append(torch.tensor(kl_score.item()))  # Convert to tensor before appending\n",
        "\n",
        "    # Return the KL divergence values obtained as Tensors\n",
        "    return kl_divergence\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Creating a Function to calculate uncertainty and diversity metrics\n",
        "def calculate_metrics(outputs, features, m=5):\n",
        "    # Calculate uncertainty metrics\n",
        "    least_confidence, prediction_entropy, margin_sampling = calculate_uncertainty_metrics(outputs.cpu().detach().numpy())\n",
        "    # Extend lists with uncertainty metrics\n",
        "    least_confidence_list.extend(torch.from_numpy(least_confidence))\n",
        "    prediction_entropy_list.extend(torch.from_numpy(prediction_entropy))\n",
        "    margin_sampling_list.extend(torch.from_numpy(margin_sampling))\n",
        "    # Calculate diversity metrics\n",
        "    features_normalized = F.normalize(features, p=2, dim=1)\n",
        "    cosine_similarity, l2_norm = calculate_diversity_metrics(features_normalized)\n",
        "    # cosine_similarity, l2_norm = calculate_diversity_metrics(features)\n",
        "    # Extend lists with diversity metrics\n",
        "    cosine_similarity_list.extend(torch.from_numpy(cosine_similarity))\n",
        "    l2_norm_list.extend(torch.from_numpy(l2_norm))\n",
        "    feature_distances = pairwise_distances(features.cpu().detach().numpy(), metric='cosine')\n",
        "    # Calculate KL divergence scores\n",
        "    kl_divergence_scores = calculate_kl_divergence(outputs, feature_distances, m=5)\n",
        "    # Extend the list with KL divergence scores\n",
        "    kl_divergence_list.extend(kl_divergence_scores)\n",
        "\n",
        "\n",
        "least_confidence_list = []\n",
        "prediction_entropy_list = []\n",
        "margin_sampling_list = []\n",
        "cosine_similarity_list = []\n",
        "l2_norm_list = []\n",
        "kl_divergence_list = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs, features = model(images)  # Get both output and features\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "        # Flatten features for further processing\n",
        "        features = features.view(features.size(0), -1)\n",
        "\n",
        "        # Calculate metrics\n",
        "        calculate_metrics(outputs, features)\n",
        "\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = correct / total\n",
        "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "# Print the average values of uncertainty and diversity measures\n",
        "print(f\"Average Least Confidence: {torch.mean(torch.stack(least_confidence_list))}\")\n",
        "print(f\"Average Prediction Entropy: {torch.mean(torch.stack(prediction_entropy_list))}\")\n",
        "print(f\"Average Margin Sampling: {torch.mean(torch.stack(margin_sampling_list))}\")\n",
        "print(f\"Average Cosine Similarity: {torch.mean(torch.stack(cosine_similarity_list))}\")\n",
        "print(f\"Average L2 Norm: {torch.mean(torch.stack(l2_norm_list))}\")\n",
        "print(f\"Average KL Divergence: {torch.mean(torch.stack(kl_divergence_list))}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2095fe7a",
      "metadata": {},
      "source": [
        "# Fourth: Using a Pretrained model LeNet5\n",
        "This model was made for MNIST dataset and we're using this for getting better accuracy<br>\n",
        "This has two convolutional layers and two pooling layers and two fully connected layers<br>\n",
        "We also calculate least confidence, prediction entropy, margin sampling, cosine similarity, l2 norm and kl divergence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "e1490141",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10, Loss: 0.25552816745961704\n",
            "Epoch 2/10, Loss: 0.07841236211778596\n",
            "Epoch 3/10, Loss: 0.056385052964215476\n",
            "Epoch 4/10, Loss: 0.04438513330162969\n",
            "Epoch 5/10, Loss: 0.03729607515376217\n",
            "Epoch 6/10, Loss: 0.03215757854296438\n",
            "Epoch 7/10, Loss: 0.02685596070534666\n",
            "Epoch 8/10, Loss: 0.023651628206616444\n",
            "Epoch 9/10, Loss: 0.020647949802401127\n",
            "Epoch 10/10, Loss: 0.017744616032129124\n",
            "Test Accuracy: 98.99%\n",
            "Average Least Confidence: 0.007267912849783897\n",
            "Average Prediction Entropy: 0.02061202935874462\n",
            "Average Margin Sampling: 0.007266698405146599\n",
            "Average Cosine Similarity: 0.35836660861968994\n",
            "Average L2 Norm: 1.1013730764389038\n",
            "Average KL Divergence: 1.622452974319458\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn.functional as F\n",
        "from sklearn.metrics import pairwise_distances\n",
        "import numpy as np\n",
        "\n",
        "# Define the LeNet-5 architecture\n",
        "# Define the LeNet-5 architecture with feature extraction\n",
        "class LeNet5WithFeatures(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(LeNet5WithFeatures, self).__init__()\n",
        "        # Define the layers\n",
        "        self.conv1 = nn.Conv2d(1, 6, kernel_size=5, stride=1, padding=0)  # First convolutional layer\n",
        "        self.pool1 = nn.AvgPool2d(kernel_size=2, stride=2)  # First pooling layer\n",
        "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5, stride=1, padding=0)  # Second convolutional layer\n",
        "        self.pool2 = nn.AvgPool2d(kernel_size=2, stride=2)  # Second pooling layer\n",
        "        self.fc1 = nn.Linear(16 * 4 * 4, 120)  # First fully connected layer\n",
        "        self.fc2 = nn.Linear(120, 84)  # Second fully connected layer\n",
        "        self.fc3 = nn.Linear(84, num_classes)  # Output layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool1(F.relu(self.conv1(x)))  # Apply conv1 -> relu -> pool1\n",
        "        x = self.pool2(F.relu(self.conv2(x)))  # Apply conv2 -> relu -> pool2\n",
        "        features = x.view(-1, 16 * 4 * 4)  # Flatten the features\n",
        "        x = F.relu(self.fc1(features))  # Apply fc1 -> relu\n",
        "        x = F.relu(self.fc2(x))  # Apply fc2 -> relu\n",
        "        output = self.fc3(x)  # Output layer\n",
        "        return output, features  # Return both output and features\n",
        "\n",
        "\n",
        "# Set the device for computations (GPU if available, otherwise CPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Define hyperparameters\n",
        "batch_size = 32\n",
        "learning_rate = 0.001\n",
        "epochs = 10\n",
        "\n",
        "# Define data transformations for preprocessing\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),  # Convert images to PyTorch tensors\n",
        "    transforms.Normalize((0.5,), (0.5,))  # Normalize for 1-channel grayscale images\n",
        "])\n",
        "\n",
        "# Download and load the MNIST dataset\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Initialize the neural network model, loss function, and optimizer\n",
        "model = LeNet5WithFeatures().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training the model\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # Forward pass\n",
        "        outputs, _ = model(images)  # Get both outputs and features\n",
        "        loss = criterion(outputs, labels)  # Use only the logits for loss computation\n",
        "        \n",
        "        # Backward pass and optimization\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    # Print average loss after each epoch\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss / len(train_loader)}\")\n",
        "\n",
        "# Evalu# Creating a function to calculate uncertainty for each epoch metrics\n",
        "def calculate_uncertainty_metrics(outputs):\n",
        "    # Convert NumPy array to PyTorch tensor\n",
        "    outputs_tensor = torch.from_numpy(outputs)\n",
        "    # Applying softmax along dimension 1\n",
        "    probabilities = F.softmax(outputs_tensor, dim=1)\n",
        "    # Least Confidence: 1 - Maximum probability for each sample\n",
        "    least_confidence = 1 - probabilities.max(dim=1).values.cpu().detach().numpy()\n",
        "    # Handling NaN in prediction entropy\n",
        "    current_probs = probabilities.clone().detach()\n",
        "    current_probs[current_probs == 0] = 1e-10  # Adding a small epsilon to avoid log(0)\n",
        "    # Prediction Entropy: Negative sum of (probability * log(probability)) for each class\n",
        "    prediction_entropy = -torch.sum(current_probs * torch.log(current_probs), dim=1).cpu().detach().numpy()\n",
        "    # Margin Sampling: 1 - (Maximum probability - Minimum probability) for each sample\n",
        "    margin_sampling = 1 - torch.max(probabilities, dim=1).values.cpu().detach().numpy() - \\\n",
        "                      torch.min(probabilities, dim=1).values.cpu().detach().numpy()\n",
        "    # Returning least confidence,prediction entropy and margin sampling values obtained\n",
        "    return least_confidence, prediction_entropy, margin_sampling\n",
        "\n",
        "# Creating a function to caalculate diversiy metrics\n",
        "def calculate_diversity_metrics(features, m=5):\n",
        "    # Calculate pairwise distances using cosine similarity\n",
        "    feature_distances = pairwise_distances(features.cpu().detach().numpy(), metric='cosine')\n",
        "    # Cosine Similarity: 1 - Mean cosine similarity with the top m neighbors for each sample\n",
        "    cosine_similarity = 1 - feature_distances[:, 1:m+1].mean(axis=1)\n",
        "    # Calculate pairwise distances using L2 (Euclidean) norm\n",
        "    l2_distances = pairwise_distances(features.cpu().detach().numpy(), metric='euclidean')\n",
        "    # L2 Norm: Mean L2 norm with the top m neighbors for each sample\n",
        "    l2_norm = l2_distances[:, 1:m+1].mean(axis=1)\n",
        "    # Returning cosine similarity and l2 norm values obtained\n",
        "    return cosine_similarity, l2_norm\n",
        "\n",
        "# # Creating a functiom to calculate kl divergence\n",
        "def calculate_kl_divergence(outputs, feature_distances, m=5):\n",
        "    # List to store KL divergence scores for each sample\n",
        "    kl_divergence = []\n",
        "\n",
        "    # Add a small epsilon to avoid log(0) and ensure numerical stability\n",
        "    epsilon = 1e-10\n",
        "\n",
        "    # Iterate over each sample in the outputs\n",
        "    for i in range(len(outputs)):\n",
        "        # Calculate the probability distribution of the current sample (softmax)\n",
        "        current_sample_prob = F.softmax(outputs[i], dim=0)\n",
        "        \n",
        "        # Add epsilon to avoid log(0) when computing log-probabilities\n",
        "        current_sample_log_prob = torch.log(current_sample_prob + epsilon)\n",
        "\n",
        "        # Get the indices of the top m neighbors for the current sample from the distance matrix\n",
        "        neighbor_indices = feature_distances[i, 1:m+1].astype(int)  # Exclude the first index, which is the sample itself\n",
        "        \n",
        "        # Calculate the average probability distribution of the neighbors\n",
        "        neighbors_prob = torch.mean(F.softmax(outputs[neighbor_indices], dim=1), dim=0)\n",
        "\n",
        "        # Add epsilon to the neighbor probabilities to avoid log(0) in KL divergence\n",
        "        neighbors_prob = neighbors_prob + epsilon\n",
        "\n",
        "        # Calculate KL divergence between the current sample and its neighbors\n",
        "        kl_score = F.kl_div(current_sample_log_prob, neighbors_prob, reduction='batchmean')\n",
        "\n",
        "        # Store the KL divergence for this sample as a Tensor\n",
        "        kl_divergence.append(torch.tensor(kl_score.item()))  # Convert to tensor before appending\n",
        "\n",
        "    # Return the KL divergence values obtained as Tensors\n",
        "    return kl_divergence\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Creating a Function to calculate uncertainty and diversity metrics\n",
        "def calculate_metrics(outputs, features, m=5):\n",
        "    # Calculate uncertainty metrics\n",
        "    least_confidence, prediction_entropy, margin_sampling = calculate_uncertainty_metrics(outputs.cpu().detach().numpy())\n",
        "    # Extend lists with uncertainty metrics\n",
        "    least_confidence_list.extend(torch.from_numpy(least_confidence))\n",
        "    prediction_entropy_list.extend(torch.from_numpy(prediction_entropy))\n",
        "    margin_sampling_list.extend(torch.from_numpy(margin_sampling))\n",
        "    # Calculate diversity metrics\n",
        "    features_normalized = F.normalize(features, p=2, dim=1)\n",
        "    cosine_similarity, l2_norm = calculate_diversity_metrics(features_normalized)\n",
        "    # cosine_similarity, l2_norm = calculate_diversity_metrics(features)\n",
        "    # Extend lists with diversity metrics\n",
        "    cosine_similarity_list.extend(torch.from_numpy(cosine_similarity))\n",
        "    l2_norm_list.extend(torch.from_numpy(l2_norm))\n",
        "    feature_distances = pairwise_distances(features.cpu().detach().numpy(), metric='cosine')\n",
        "    # Calculate KL divergence scores\n",
        "    kl_divergence_scores = calculate_kl_divergence(outputs, feature_distances, m=5)\n",
        "    # Extend the list with KL divergence scores\n",
        "    kl_divergence_list.extend(kl_divergence_scores)\n",
        "\n",
        "\n",
        "# Evaluating the model and calculating metrics\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "# Reset metric lists at the start of each evaluation\n",
        "least_confidence_list = []\n",
        "prediction_entropy_list = []\n",
        "margin_sampling_list = []\n",
        "cosine_similarity_list = []\n",
        "l2_norm_list = []\n",
        "kl_divergence_list = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        \n",
        "        # Forward pass\n",
        "        outputs, features = model(images)  # Extract both outputs and features\n",
        "        _, predicted = torch.max(outputs, 1)  # Use only the logits for predictions\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "        features = features.view(features.size(0),-1)\n",
        "        calculate_metrics(outputs, features)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = correct / total\n",
        "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "# Print the average values of uncertainty and diversity measures\n",
        "print(f\"Average Least Confidence: {torch.mean(torch.stack(least_confidence_list))}\")\n",
        "print(f\"Average Prediction Entropy: {torch.mean(torch.stack(prediction_entropy_list))}\")\n",
        "print(f\"Average Margin Sampling: {torch.mean(torch.stack(margin_sampling_list))}\")\n",
        "print(f\"Average Cosine Similarity: {torch.mean(torch.stack(cosine_similarity_list))}\")\n",
        "print(f\"Average L2 Norm: {torch.mean(torch.stack(l2_norm_list))}\")\n",
        "print(f\"Average KL Divergence: {torch.mean(torch.stack(kl_divergence_list))}\")\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
