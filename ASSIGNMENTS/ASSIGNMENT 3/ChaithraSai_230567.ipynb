{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from sklearn.model_selection import train_test_split\n",
        "from scipy.stats import entropy\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load and preprocess data\n",
        "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
        "X_train_full = X_train_full / 255.0\n",
        "X_test = X_test / 255.0\n",
        "\n",
        "# Split training data into training and validation sets\n",
        "X_train, X_validation, y_train, y_validation = train_test_split(\n",
        "    X_train_full, y_train_full, test_size=0.2, random_state=2020\n",
        ")\n",
        "\n",
        "# Expand dimensions for channels\n",
        "X_train = np.expand_dims(X_train, -1)\n",
        "X_validation = np.expand_dims(X_validation, -1)\n",
        "X_test = np.expand_dims(X_test, -1)\n",
        "\n",
        "# Define the CNN model\n",
        "def create_model():\n",
        "    model = keras.Sequential([\n",
        "        layers.Conv2D(32, kernel_size=3, activation='relu', input_shape=(28, 28, 1)),\n",
        "        layers.MaxPooling2D(pool_size=2),\n",
        "        layers.Conv2D(64, kernel_size=3, activation='relu'),\n",
        "        layers.MaxPooling2D(pool_size=2),\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(128, activation='relu'),\n",
        "        layers.Dropout(0.25),\n",
        "        layers.Dense(10, activation='softmax')\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Metrics Calculation\n",
        "def calculate_metrics(model, X_pool):\n",
        "    predictions = model.predict(X_pool, batch_size=256)\n",
        "    features = model.predict(X_pool, batch_size=512)\n",
        "\n",
        "    # Least Confidence\n",
        "    least_conf_scores = np.max(predictions, axis=1)\n",
        "    avg_least_confidence = np.mean(least_conf_scores)\n",
        "\n",
        "    # Prediction Entropy\n",
        "    pred_entropies = entropy(predictions.T)\n",
        "    avg_prediction_entropy = np.mean(pred_entropies)\n",
        "\n",
        "    # Margin Sampling\n",
        "    sorted_preds = -np.sort(-predictions, axis=1)\n",
        "    margins = sorted_preds[:, 0] - sorted_preds[:, 1]\n",
        "    avg_margin_sampling = np.mean(margins)\n",
        "\n",
        "    # Cosine Similarity (Feature Diversity)\n",
        "    similarities = cosine_similarity(features)\n",
        "    diversities = 1 - similarities.sum(axis=1)\n",
        "    avg_cosine_similarity = np.mean(diversities)\n",
        "\n",
        "    # L2 Norm\n",
        "    l2_norms = np.linalg.norm(features, axis=1)\n",
        "    avg_l2_norm = np.mean(l2_norms)\n",
        "\n",
        "    # KL Divergence\n",
        "    uniform_dist = np.ones_like(predictions) / predictions.shape[1]\n",
        "    kl_divergences = np.sum(predictions * np.log(predictions / uniform_dist), axis=1)\n",
        "    avg_kl_divergence = np.mean(kl_divergences)\n",
        "\n",
        "    return {\n",
        "        \"Average Least Confidence\": avg_least_confidence,\n",
        "        \"Average Prediction Entropy\": avg_prediction_entropy,\n",
        "        \"Average Margin Sampling\": avg_margin_sampling,\n",
        "        \"Average Cosine Similarity\": avg_cosine_similarity,\n",
        "        \"Average L2 Norm\": avg_l2_norm,\n",
        "        \"Average KL Divergence\": avg_kl_divergence\n",
        "    }\n",
        "\n",
        "# Train model without active learning\n",
        "model_without_al = create_model()\n",
        "history_without_al = model_without_al.fit(\n",
        "    X_train, y_train, epochs=20, batch_size=512, validation_data=(X_validation, y_validation)\n",
        ")\n",
        "val_acc_without_al = model_without_al.evaluate(X_validation, y_validation, verbose=0)[1]\n",
        "print(f'Validation Accuracy without Active Learning: {val_acc_without_al:.2f}')\n",
        "\n",
        "# Active learning selection\n",
        "n_samples = 1000\n",
        "selected_indices = np.random.choice(X_validation.shape[0], n_samples, replace=False)  # Random selection\n",
        "X_selected = X_validation[selected_indices]\n",
        "y_selected = y_validation[selected_indices]\n",
        "\n",
        "# Augment training data with selected samples\n",
        "X_train_al = np.concatenate([X_train, X_selected], axis=0)\n",
        "y_train_al = np.concatenate([y_train, y_selected], axis=0)\n",
        "\n",
        "# Train model with active learning\n",
        "model_with_al = create_model()\n",
        "history_with_al = model_with_al.fit(\n",
        "    X_train_al, y_train_al, epochs=25, batch_size=256, validation_data=(X_validation, y_validation)\n",
        ")\n",
        "val_acc_with_al = model_with_al.evaluate(X_validation, y_validation, verbose=0)[1]\n",
        "print(f'Validation Accuracy with Active Learning: {val_acc_with_al:.2f}')\n",
        "\n",
        "# Calculate metrics for the validation pool\n",
        "metrics_before_al = calculate_metrics(model_without_al, X_validation)\n",
        "metrics_after_al = calculate_metrics(model_with_al, X_validation)\n",
        "\n",
        "# Print metrics\n",
        "print(\"\\nMetrics Before Active Learning:\")\n",
        "for metric, value in metrics_before_al.items():\n",
        "    print(f\"{metric}: {value:.4f}\")\n",
        "\n",
        "print(\"\\nMetrics After Active Learning:\")\n",
        "for metric, value in metrics_after_al.items():\n",
        "    print(f\"{metric}: {value:.4f}\")\n",
        "\n",
        "# Plot accuracy graphs\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(history_without_al.history['val_accuracy'], label='Without Active Learning', linestyle='--')\n",
        "plt.plot(history_with_al.history['val_accuracy'], label='With Active Learning', linestyle='-')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Validation Accuracy')\n",
        "plt.title('Validation Accuracy With and Without Active Learning')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pd85UCLhFZHO",
        "outputId": "c5f090bf-58c7-4c14-9b16-1354353d3d51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "\u001b[1m29515/29515\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "\u001b[1m26421880/26421880\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "\u001b[1m5148/5148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "\u001b[1m4422102/4422102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m86/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m3s\u001b[0m 477ms/step - accuracy: 0.5532 - loss: 1.2983"
          ]
        }
      ]
    }
  ]
}