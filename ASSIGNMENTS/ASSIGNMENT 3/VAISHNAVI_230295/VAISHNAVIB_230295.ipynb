{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "ASSIGNMENT 3"
      ],
      "metadata": {
        "id": "uTY4BWlQPU-Q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "WoxTX-_dPR_Q"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import FashionMNIST\n",
        "from scipy.stats import entropy\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data preprocessing\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n"
      ],
      "metadata": {
        "id": "thoELw6fPsMc"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "train_dataset = FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = FashionMNIST(root='./data', train=False, download=True, transform=transform)\n"
      ],
      "metadata": {
        "id": "CG3TjqMAP01b"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the training dataset\n",
        "X_train, X_validation, y_train, y_validation = train_test_split(train_dataset.data.numpy(),\n",
        "                                                              train_dataset.targets.numpy(),\n",
        "                                                              test_size=0.2, random_state=2020)\n"
      ],
      "metadata": {
        "id": "ClOrU79kP-OK"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = torch.tensor(X_train, dtype=torch.float32).unsqueeze(1) / 255\n",
        "X_validation = torch.tensor(X_validation, dtype=torch.float32).unsqueeze(1) / 255\n",
        "y_train = torch.tensor(y_train, dtype=torch.long)\n",
        "y_validation = torch.tensor(y_validation, dtype=torch.long)\n",
        "\n"
      ],
      "metadata": {
        "id": "YcbT7_i2QHUT"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test = torch.tensor(test_dataset.data.numpy(), dtype=torch.float32).unsqueeze(1) / 255\n",
        "y_test = torch.tensor(test_dataset.targets.numpy(), dtype=torch.long)"
      ],
      "metadata": {
        "id": "EW4PQHdkQMMZ"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create data loaders\n",
        "def create_data_loader(X, y, batch_size):\n",
        "    return DataLoader(TensorDataset(X, y), batch_size=batch_size, shuffle=True)\n"
      ],
      "metadata": {
        "id": "bdGpiNVCQQOR"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = create_data_loader(X_train, y_train, batch_size=512)\n",
        "validation_loader = create_data_loader(X_validation, y_validation, batch_size=512)\n",
        "test_loader = create_data_loader(X_test, y_test, batch_size=512)\n",
        "\n"
      ],
      "metadata": {
        "id": "uPg7yXMWQULV"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CNNModel(nn.Module):\n",
        "    def __init__(self): # Corrected the method name to __init__\n",
        "        super(CNNModel, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.fc1 = nn.Linear(64 * 5 * 5, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(torch.relu(self.conv1(x)))\n",
        "        x = self.pool(torch.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 64 * 5 * 5)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "1SOYMybTSqfv"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define training function\n",
        "def train_model(model, train_loader, criterion, optimizer, device, epochs):\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Backward pass\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        print(f'Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(train_loader)}')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "BD916sStQiPc"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define evaluation function\n",
        "def evaluate_model(model, data_loader, device):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in data_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    return 100 * correct / total\n",
        "\n"
      ],
      "metadata": {
        "id": "Ukh1JN_fQmLW"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Active Learning - Uncertainty Metrics\n",
        "def least_confidence(predictions):\n",
        "    confidences, _ = predictions.max(dim=1)\n",
        "    return confidences.argsort()\n",
        "\n"
      ],
      "metadata": {
        "id": "_hNvjZ3WRJ0S"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prediction_entropy(predictions):\n",
        "    probabilities = torch.softmax(predictions, dim=1)\n",
        "    entropies = entropy(probabilities.cpu().numpy(), axis=1)\n",
        "    return torch.tensor(entropies).argsort(descending=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "au4Lh_F7ROmq"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def margin_sampling(predictions):\n",
        "    probabilities = torch.softmax(predictions, dim=1)\n",
        "    top2_probs, _ = probabilities.topk(2, dim=1)\n",
        "    margins = top2_probs[:, 0] - top2_probs[:, 1]\n",
        "    return margins.argsort()\n"
      ],
      "metadata": {
        "id": "qdF5ZrxiRTVi"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Active Learning - Diversity Metrics\n",
        "def cosine_similarity_metric(features):\n",
        "    similarities = cosine_similarity(features.cpu().numpy())\n",
        "    diversities = 1 - similarities.sum(axis=1)\n",
        "    return torch.tensor(diversities).argsort(descending=True)\n"
      ],
      "metadata": {
        "id": "2CWR5HnJRWit"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def l2_norm_metric(features):\n",
        "    distances = torch.cdist(features, features, p=2).sum(dim=1)\n",
        "    return distances.argsort(descending=True)\n"
      ],
      "metadata": {
        "id": "FobV_-ukRaJf"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def kl_divergence_metric(predictions):\n",
        "    probabilities = torch.softmax(predictions, dim=1)\n",
        "    mean_distribution = probabilities.mean(dim=0)\n",
        "    kl_divs = entropy(probabilities.cpu().numpy().T, mean_distribution.cpu().numpy())\n",
        "    return torch.tensor(kl_divs).argsort(descending=True)\n"
      ],
      "metadata": {
        "id": "jijU-p2ARdva"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train and evaluate without active learning\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model_without_al = CNNModel().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model_without_al.parameters(), lr=0.001)\n",
        "\n",
        "print(\"Training without Active Learning:\")\n",
        "train_model(model_without_al, train_loader, criterion, optimizer, device, epochs=30)\n",
        "validation_accuracy_without_al = evaluate_model(model_without_al, validation_loader, device)\n",
        "print(f'Validation Accuracy without Active Learning: {validation_accuracy_without_al}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qAi-uPlOSR2t",
        "outputId": "ca761e66-653e-4c17-ea3e-d7072a33b0db"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training without Active Learning:\n",
            "Epoch 1/30, Loss: 0.9231703110197758\n",
            "Epoch 2/30, Loss: 0.5347481259006135\n",
            "Epoch 3/30, Loss: 0.45318579356721106\n",
            "Epoch 4/30, Loss: 0.40938833324199025\n",
            "Epoch 5/30, Loss: 0.37749939713072267\n",
            "Epoch 6/30, Loss: 0.3543104405732865\n",
            "Epoch 7/30, Loss: 0.33810041559503434\n",
            "Epoch 8/30, Loss: 0.3222665615538333\n",
            "Epoch 9/30, Loss: 0.3068042335675118\n",
            "Epoch 10/30, Loss: 0.29680865321387634\n",
            "Epoch 11/30, Loss: 0.2895380012849544\n",
            "Epoch 12/30, Loss: 0.2801765360413714\n",
            "Epoch 13/30, Loss: 0.27411900381458565\n",
            "Epoch 14/30, Loss: 0.2668853982331905\n",
            "Epoch 15/30, Loss: 0.25348374199994067\n",
            "Epoch 16/30, Loss: 0.25181598913796405\n",
            "Epoch 17/30, Loss: 0.2429173048189346\n",
            "Epoch 18/30, Loss: 0.23320344042904834\n",
            "Epoch 19/30, Loss: 0.22877632129065534\n",
            "Epoch 20/30, Loss: 0.22708501270476808\n",
            "Epoch 21/30, Loss: 0.21764474695033215\n",
            "Epoch 22/30, Loss: 0.21279376206245829\n",
            "Epoch 23/30, Loss: 0.2087788684888089\n",
            "Epoch 24/30, Loss: 0.20324456802708038\n",
            "Epoch 25/30, Loss: 0.1956401121743182\n",
            "Epoch 26/30, Loss: 0.18849890726677915\n",
            "Epoch 27/30, Loss: 0.18502891444145364\n",
            "Epoch 28/30, Loss: 0.18174555707485118\n",
            "Epoch 29/30, Loss: 0.17738703376752266\n",
            "Epoch 30/30, Loss: 0.17852425416733356\n",
            "Validation Accuracy without Active Learning: 90.14166666666667%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Active learning implementation\n",
        "def active_learning_selection(model, loader, device, selection_method, n_samples=100):\n",
        "    model.eval()\n",
        "    all_indices = []\n",
        "    all_scores = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            if selection_method in [least_confidence, prediction_entropy, margin_sampling]:\n",
        "                scores = selection_method(outputs)\n",
        "            else:\n",
        "                features = inputs.view(inputs.size(0), -1)\n",
        "                scores = selection_method(features)\n",
        "\n",
        "            all_indices.extend(scores.cpu().numpy())\n",
        "            all_scores.extend(scores.cpu().numpy())\n",
        "\n",
        "    selected_indices = np.argsort(all_scores)[:n_samples]\n",
        "    return selected_indices\n",
        "\n"
      ],
      "metadata": {
        "id": "qVGNiTQ8VLX6"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Select additional samples using active learning\n",
        "selected_indices = active_learning_selection(model_without_al, validation_loader, device, least_confidence, n_samples=500)\n",
        "X_selected = X_validation[selected_indices]\n",
        "y_selected = y_validation[selected_indices]\n",
        "\n"
      ],
      "metadata": {
        "id": "HNH_GTqKVR7y"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine selected samples with training data\n",
        "X_train_al = torch.cat([X_train, X_selected])\n",
        "y_train_al = torch.cat([y_train, y_selected])\n",
        "train_loader_al = create_data_loader(X_train_al, y_train_al, batch_size=512)\n"
      ],
      "metadata": {
        "id": "B1px6PqFVTOF"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train and evaluate with active learning\n",
        "model_with_al = CNNModel().to(device)\n",
        "optimizer_al = optim.Adam(model_with_al.parameters(), lr=0.001)\n"
      ],
      "metadata": {
        "id": "RioJ41eQVZL0"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Training with Active Learning:\")\n",
        "train_model(model_with_al, train_loader_al, criterion, optimizer_al, device, epochs=30)\n",
        "validation_accuracy_with_al = evaluate_model(model_with_al, validation_loader, device)\n",
        "print(f'Validation Accuracy with Active Learning: {validation_accuracy_with_al}%')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zc-uJtqZVeCi",
        "outputId": "a0727642-4a6c-4db4-ef3a-00a29e686980"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training with Active Learning:\n",
            "Epoch 1/30, Loss: 0.1965095927840785\n",
            "Epoch 2/30, Loss: 0.18978847657379352\n",
            "Epoch 3/30, Loss: 0.18232213478339346\n",
            "Epoch 4/30, Loss: 0.1826060491172891\n",
            "Epoch 5/30, Loss: 0.17295636103341455\n",
            "Epoch 6/30, Loss: 0.17251491154495038\n",
            "Epoch 7/30, Loss: 0.1646222642377803\n",
            "Epoch 8/30, Loss: 0.15898365629346747\n",
            "Epoch 9/30, Loss: 0.15337324911042263\n",
            "Epoch 10/30, Loss: 0.1522122110975416\n",
            "Epoch 11/30, Loss: 0.14168761145127448\n",
            "Epoch 12/30, Loss: 0.139103681162784\n",
            "Epoch 13/30, Loss: 0.13911132412521462\n",
            "Epoch 14/30, Loss: 0.1281212761213905\n",
            "Epoch 15/30, Loss: 0.12801117567639603\n",
            "Epoch 16/30, Loss: 0.1246933693948545\n",
            "Epoch 17/30, Loss: 0.11523377385578658\n",
            "Epoch 18/30, Loss: 0.10865355039897719\n",
            "Epoch 19/30, Loss: 0.11958260340125937\n",
            "Epoch 20/30, Loss: 0.10045395540563684\n",
            "Epoch 21/30, Loss: 0.0991326284251715\n",
            "Epoch 22/30, Loss: 0.09623174875190384\n",
            "Epoch 23/30, Loss: 0.08864146976878769\n",
            "Epoch 24/30, Loss: 0.08528193082464369\n",
            "Epoch 25/30, Loss: 0.0857880573523672\n",
            "Epoch 26/30, Loss: 0.08188469441313492\n",
            "Epoch 27/30, Loss: 0.07653305240367589\n",
            "Epoch 28/30, Loss: 0.07624843830340787\n",
            "Epoch 29/30, Loss: 0.06949221825129108\n",
            "Epoch 30/30, Loss: 0.06355636602169588\n",
            "Validation Accuracy with Active Learning: 91.05%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save models for comparison\n",
        "torch.save(model_without_al.state_dict(), 'model_without_al.pth')\n",
        "torch.save(model_with_al.state_dict(), 'model_with_al.pth')"
      ],
      "metadata": {
        "id": "SEahYK8sXzFO"
      },
      "execution_count": 74,
      "outputs": []
    }
  ]
}