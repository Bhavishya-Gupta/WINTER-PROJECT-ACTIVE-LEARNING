{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "sZSQCY45xf0E"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "from IPython import get_ipython\n",
        "from IPython.display import display\n",
        "from sklearn.metrics.pairwise import pairwise_distances\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "import copy\n",
        "\n",
        "\n",
        "\n",
        "def active_learning_iteration(model, full_dataset, strategy, num_iterations, query_size, epochs_per_iteration, learning_rate=0.001):\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    initial_labeled_size = 2000\n",
        "    labeled_indices = list(range(initial_labeled_size))\n",
        "    unlabeled_indices = list(range(initial_labeled_size, len(full_dataset)))\n",
        "\n",
        "    labeled_dataset = Subset(full_dataset, labeled_indices)\n",
        "\n",
        "    unlabeled_dataset = Subset(full_dataset, unlabeled_indices)\n",
        "    labeled_dataset = Subset(full_dataset, labeled_indices)\n",
        "    for iteration in range(num_iterations):\n",
        "        print(f\"Iteration {iteration + 1}\")\n",
        "\n",
        "        unlabeled_subset = Subset(full_dataset, unlabeled_indices)\n",
        "        unlabeled_loader = DataLoader(unlabeled_subset, batch_size=64, shuffle=False)\n",
        "\n",
        "        model.eval()\n",
        "        outputs_list = []\n",
        "        features_list = []\n",
        "        with torch.no_grad():\n",
        "            for images, _ in unlabeled_loader:\n",
        "                outputs, features = model(images)\n",
        "                outputs_list.append(outputs)\n",
        "                features_list.append(features)\n",
        "        outputs = torch.cat(outputs_list, dim=0)\n",
        "        features = torch.cat(features_list, dim = 0)\n",
        "\n",
        "\n",
        "\n",
        "        if strategy == margin_sampling:\n",
        "            result = strategy(outputs)\n",
        "            result = np.ascontiguousarray(result)\n",
        "            query_indices = np.flip(np.argsort(result))[:query_size]\n",
        "\n",
        "        elif strategy == prediction_entropy or strategy == least_confidence:\n",
        "\n",
        "            result = strategy(outputs)\n",
        "\n",
        "            result = np.ascontiguousarray(result)\n",
        "            query_indices = np.flip(np.argsort(result))[:query_size]\n",
        "\n",
        "        elif strategy == calculate_kl_divergence:\n",
        "            feature_distances = pairwise_distances(features.cpu().detach().numpy(), metric='cosine')\n",
        "            result = strategy(outputs, feature_distances, 5)\n",
        "            query_indices = np.flip(np.argsort(result))[:query_size]\n",
        "\n",
        "\n",
        "        elif strategy == L2  or strategy ==  cosine_similarity:\n",
        "            result = strategy(features, 5)\n",
        "            query_indices = np.argsort(result)[:query_size]\n",
        "\n",
        "\n",
        "\n",
        "        selected_unlabeled_indices = [unlabeled_indices[i] for i in query_indices]\n",
        "\n",
        "        labeled_indices.extend(selected_unlabeled_indices)\n",
        "        unlabeled_indices = [idx for idx in unlabeled_indices if idx not in selected_unlabeled_indices]\n",
        "\n",
        "        updated_labeled_dataset = Subset(full_dataset, labeled_indices)\n",
        "        updated_labeled_loader = DataLoader(updated_labeled_dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "        model.train()\n",
        "        for epoch in range(epochs_per_iteration):\n",
        "            running_loss = 0.0\n",
        "            for images, labels in updated_labeled_loader:\n",
        "                optimizer.zero_grad()\n",
        "                outputs, _ = model(images)\n",
        "                loss = criterion(outputs, labels)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                running_loss += loss.item()\n",
        "\n",
        "            print(f\"Epoch {epoch + 1}, Loss: {running_loss / len(updated_labeled_loader):.4f}\")\n",
        "\n",
        "    return len(labeled_indices)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "K2e2k1Ytxvwy"
      },
      "outputs": [],
      "source": [
        "class CNNModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNNModel, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3)\n",
        "        self.fc1 = nn.Linear(64 * 6 * 6, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
        "        out  = F.relu(F.max_pool2d(self.conv2(out), 2))\n",
        "        out = out.view(out.size(0), -1)\n",
        "        features  = F.relu(self.fc1(out))\n",
        "        out = self.fc2(features)\n",
        "        features = features.view(features.size(0), -1)\n",
        "        return out,features\n",
        "\n",
        "query_size = 300\n",
        "epochs_per_iteration = 5\n",
        "num_iterations = 10\n",
        "model = CNNModel()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XmxaCBP37Fjj"
      },
      "source": [
        "Functions For Uncertainty-based Querying Strategies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "collapsed": true,
        "id": "np2Fj3xOypnl"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def least_confidence(outputs):\n",
        "    probabilities = torch.softmax(outputs, dim=1)\n",
        "    max_probabilities, _ = torch.max(probabilities, dim=1)\n",
        "    least_conf = 1 - max_probabilities\n",
        "    return least_conf.cpu().numpy()\n",
        "def prediction_entropy(outputs):\n",
        "    probabilities = F.softmax(outputs, dim=1)\n",
        "    probabilities = torch.clamp(probabilities, min=1e-7, max=1.0)\n",
        "    log_probabilities = torch.log(probabilities)\n",
        "    entropy = -torch.sum(probabilities * log_probabilities, dim=1)\n",
        "    return entropy\n",
        "def margin_sampling(outputs):\n",
        "    probabilities = F.softmax(outputs, dim=1)\n",
        "    top_two_probs, _ = torch.topk(probabilities, 2, dim=1)\n",
        "    margin = (top_two_probs[:,0] - top_two_probs[:,1]).detach().cpu().numpy()\n",
        "    return margin\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mky8lVe59d-h"
      },
      "source": [
        "Loading CIFAR10 Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oQe5DiQ63mas",
        "outputId": "75623322-280f-43b6-e4aa-eb659156afa0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:05<00:00, 29.4MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9OFeP4J_-CMS"
      },
      "source": [
        "Least Confidence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wDtBOj4g4eLA",
        "outputId": "3150d0bd-13cd-4aa3-cf09-7be74a5e731c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1\n",
            "Epoch 1, Loss: 2.0833\n",
            "Epoch 2, Loss: 1.7783\n",
            "Epoch 3, Loss: 1.6112\n",
            "Epoch 4, Loss: 1.4485\n",
            "Epoch 5, Loss: 1.3797\n",
            "Iteration 2\n",
            "Epoch 1, Loss: 1.3735\n",
            "Epoch 2, Loss: 1.2788\n",
            "Epoch 3, Loss: 1.1869\n",
            "Epoch 4, Loss: 1.0765\n",
            "Epoch 5, Loss: 0.9767\n",
            "Iteration 3\n",
            "Epoch 1, Loss: 1.0277\n",
            "Epoch 2, Loss: 0.8846\n",
            "Epoch 3, Loss: 0.7957\n",
            "Epoch 4, Loss: 0.7025\n",
            "Epoch 5, Loss: 0.5844\n",
            "Iteration 4\n",
            "Epoch 1, Loss: 0.6738\n",
            "Epoch 2, Loss: 0.5541\n",
            "Epoch 3, Loss: 0.4252\n",
            "Epoch 4, Loss: 0.3564\n",
            "Epoch 5, Loss: 0.2725\n",
            "Iteration 5\n",
            "Epoch 1, Loss: 0.4400\n",
            "Epoch 2, Loss: 0.3213\n",
            "Epoch 3, Loss: 0.2307\n",
            "Epoch 4, Loss: 0.1850\n",
            "Epoch 5, Loss: 0.1281\n",
            "Iteration 6\n",
            "Epoch 1, Loss: 0.3252\n",
            "Epoch 2, Loss: 0.2297\n",
            "Epoch 3, Loss: 0.1544\n",
            "Epoch 4, Loss: 0.0891\n",
            "Epoch 5, Loss: 0.0569\n",
            "Iteration 7\n",
            "Epoch 1, Loss: 0.3826\n",
            "Epoch 2, Loss: 0.2276\n",
            "Epoch 3, Loss: 0.1079\n",
            "Epoch 4, Loss: 0.0632\n",
            "Epoch 5, Loss: 0.0581\n",
            "Iteration 8\n",
            "Epoch 1, Loss: 0.3315\n",
            "Epoch 2, Loss: 0.1620\n",
            "Epoch 3, Loss: 0.0777\n",
            "Epoch 4, Loss: 0.0410\n",
            "Epoch 5, Loss: 0.0211\n",
            "Iteration 9\n",
            "Epoch 1, Loss: 0.3174\n",
            "Epoch 2, Loss: 0.1544\n",
            "Epoch 3, Loss: 0.0789\n",
            "Epoch 4, Loss: 0.0427\n",
            "Epoch 5, Loss: 0.0168\n",
            "Iteration 10\n",
            "Epoch 1, Loss: 0.3098\n",
            "Epoch 2, Loss: 0.2000\n",
            "Epoch 3, Loss: 0.0756\n",
            "Epoch 4, Loss: 0.0298\n",
            "Epoch 5, Loss: 0.0109\n",
            "Test Accuracy: 54.64%\n",
            "Total Training Instances Used : 5000\n"
          ]
        }
      ],
      "source": [
        "\n",
        "model = CNNModel()\n",
        "model.eval()\n",
        "labeled_indices = active_learning_iteration(\n",
        "    model, train_dataset, least_confidence, num_iterations, query_size, epochs_per_iteration\n",
        ")\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "total = 0\n",
        "correct = 0\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        outputs, _  = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f\"Test Accuracy: {100 * correct / total:.2f}%\")\n",
        "print(f\"Total Training Instances Used :\" ,labeled_indices)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3b-JDYsb-Liy"
      },
      "source": [
        "Prediction Entropy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f_fZTixO-K56",
        "outputId": "27cbc0ff-3c8b-44bb-b0af-f326fc679437"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1\n",
            "Epoch 1, Loss: 2.0829\n",
            "Epoch 2, Loss: 1.7789\n",
            "Epoch 3, Loss: 1.5826\n",
            "Epoch 4, Loss: 1.4435\n",
            "Epoch 5, Loss: 1.3333\n",
            "Iteration 2\n",
            "Epoch 1, Loss: 1.3697\n",
            "Epoch 2, Loss: 1.2910\n",
            "Epoch 3, Loss: 1.2296\n",
            "Epoch 4, Loss: 1.1290\n",
            "Epoch 5, Loss: 1.0451\n",
            "Iteration 3\n",
            "Epoch 1, Loss: 1.1030\n",
            "Epoch 2, Loss: 1.0152\n",
            "Epoch 3, Loss: 0.8866\n",
            "Epoch 4, Loss: 0.8151\n",
            "Epoch 5, Loss: 0.7134\n",
            "Iteration 4\n",
            "Epoch 1, Loss: 0.8006\n",
            "Epoch 2, Loss: 0.7032\n",
            "Epoch 3, Loss: 0.5737\n",
            "Epoch 4, Loss: 0.5087\n",
            "Epoch 5, Loss: 0.4154\n",
            "Iteration 5\n",
            "Epoch 1, Loss: 0.5631\n",
            "Epoch 2, Loss: 0.4512\n",
            "Epoch 3, Loss: 0.3582\n",
            "Epoch 4, Loss: 0.2914\n",
            "Epoch 5, Loss: 0.2162\n",
            "Iteration 6\n",
            "Epoch 1, Loss: 0.4279\n",
            "Epoch 2, Loss: 0.2989\n",
            "Epoch 3, Loss: 0.2254\n",
            "Epoch 4, Loss: 0.1706\n",
            "Epoch 5, Loss: 0.0990\n",
            "Iteration 7\n",
            "Epoch 1, Loss: 0.3372\n",
            "Epoch 2, Loss: 0.3072\n",
            "Epoch 3, Loss: 0.1557\n",
            "Epoch 4, Loss: 0.1803\n",
            "Epoch 5, Loss: 0.0816\n",
            "Iteration 8\n",
            "Epoch 1, Loss: 0.3220\n",
            "Epoch 2, Loss: 0.2125\n",
            "Epoch 3, Loss: 0.1229\n",
            "Epoch 4, Loss: 0.0680\n",
            "Epoch 5, Loss: 0.0423\n",
            "Iteration 9\n",
            "Epoch 1, Loss: 0.3202\n",
            "Epoch 2, Loss: 0.2040\n",
            "Epoch 3, Loss: 0.1318\n",
            "Epoch 4, Loss: 0.0737\n",
            "Epoch 5, Loss: 0.0342\n",
            "Iteration 10\n",
            "Epoch 1, Loss: 0.2866\n",
            "Epoch 2, Loss: 0.1632\n",
            "Epoch 3, Loss: 0.0606\n",
            "Epoch 4, Loss: 0.0303\n",
            "Epoch 5, Loss: 0.0154\n",
            "Test Accuracy: 55.29%\n",
            "Total Training Instances Used : 5000\n"
          ]
        }
      ],
      "source": [
        "model = CNNModel()\n",
        "model.eval()\n",
        "labeled_indices = active_learning_iteration(\n",
        "    model, train_dataset, prediction_entropy, num_iterations, query_size, epochs_per_iteration\n",
        ")\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "total = 0\n",
        "correct = 0\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        outputs,_  = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f\"Test Accuracy: {100 * correct / total:.2f}%\")\n",
        "print(f\"Total Training Instances Used :\" ,labeled_indices)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9rm6tj1cUX74"
      },
      "source": [
        "Margin Sampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vyrfdMpl-cmT",
        "outputId": "0fca998c-6edf-4606-9e74-b74d33ca1df5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1\n",
            "Epoch 1, Loss: 2.0990\n",
            "Epoch 2, Loss: 1.7814\n",
            "Epoch 3, Loss: 1.5891\n",
            "Epoch 4, Loss: 1.4440\n",
            "Epoch 5, Loss: 1.3524\n",
            "Iteration 2\n",
            "Epoch 1, Loss: 1.2167\n",
            "Epoch 2, Loss: 1.1153\n",
            "Epoch 3, Loss: 1.0227\n",
            "Epoch 4, Loss: 0.9509\n",
            "Epoch 5, Loss: 0.8550\n",
            "Iteration 3\n",
            "Epoch 1, Loss: 0.7817\n",
            "Epoch 2, Loss: 0.6998\n",
            "Epoch 3, Loss: 0.5990\n",
            "Epoch 4, Loss: 0.5319\n",
            "Epoch 5, Loss: 0.4410\n",
            "Iteration 4\n",
            "Epoch 1, Loss: 0.3831\n",
            "Epoch 2, Loss: 0.3439\n",
            "Epoch 3, Loss: 0.2741\n",
            "Epoch 4, Loss: 0.2344\n",
            "Epoch 5, Loss: 0.1610\n",
            "Iteration 5\n",
            "Epoch 1, Loss: 0.1781\n",
            "Epoch 2, Loss: 0.1299\n",
            "Epoch 3, Loss: 0.0863\n",
            "Epoch 4, Loss: 0.0610\n",
            "Epoch 5, Loss: 0.0370\n",
            "Iteration 6\n",
            "Epoch 1, Loss: 0.2149\n",
            "Epoch 2, Loss: 0.1225\n",
            "Epoch 3, Loss: 0.0691\n",
            "Epoch 4, Loss: 0.0378\n",
            "Epoch 5, Loss: 0.0211\n",
            "Iteration 7\n",
            "Epoch 1, Loss: 0.1811\n",
            "Epoch 2, Loss: 0.1140\n",
            "Epoch 3, Loss: 0.0527\n",
            "Epoch 4, Loss: 0.0307\n",
            "Epoch 5, Loss: 0.0135\n",
            "Iteration 8\n",
            "Epoch 1, Loss: 0.1906\n",
            "Epoch 2, Loss: 0.0705\n",
            "Epoch 3, Loss: 0.0354\n",
            "Epoch 4, Loss: 0.0130\n",
            "Epoch 5, Loss: 0.0078\n",
            "Iteration 9\n",
            "Epoch 1, Loss: 0.1685\n",
            "Epoch 2, Loss: 0.0714\n",
            "Epoch 3, Loss: 0.0340\n",
            "Epoch 4, Loss: 0.0136\n",
            "Epoch 5, Loss: 0.0062\n",
            "Iteration 10\n",
            "Epoch 1, Loss: 0.1429\n",
            "Epoch 2, Loss: 0.0586\n",
            "Epoch 3, Loss: 0.0305\n",
            "Epoch 4, Loss: 0.0083\n",
            "Epoch 5, Loss: 0.0031\n",
            "Test Accuracy: 49.93%\n",
            "Total Training Instances Used : 5000\n"
          ]
        }
      ],
      "source": [
        "model = CNNModel()\n",
        "model.eval()\n",
        "labeled_indices = active_learning_iteration(\n",
        "    model, train_dataset, margin_sampling, num_iterations, query_size, epochs_per_iteration\n",
        ")\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "total = 0\n",
        "correct = 0\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        outputs,_  = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "print(f\"Test Accuracy: {100 * correct / total:.2f}%\")\n",
        "print(f\"Total Training Instances Used :\" ,labeled_indices)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_gYELGuWUI22"
      },
      "source": [
        "I have implemented the functions for diversity-metrics but it was showing RAM ran out of memory error. I have tried to fix it multiple times but have not succeeded.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Buu246WnlWZ-"
      },
      "outputs": [],
      "source": [
        "\n",
        "# from sklearn.metrics import pairwise_distances\n",
        "\n",
        "# def L2(features, m=5):\n",
        "#     # Compute pairwise Euclidean distances\n",
        "#     l2_distances = pairwise_distances(features.cpu().detach().numpy(), metric='euclidean')\n",
        "#     l2_norm = l2_distances[:, 1:m+1].mean(axis=1)\n",
        "#     return l2_norm\n",
        "\n",
        "# def cosine_similarity(features, m=5):\n",
        "#     features_normalized = F.normalize(features, p=2, dim=1)\n",
        "#     feature_distances = pairwise_distances(features_normalized.cpu().detach().numpy(), metric='cosine')\n",
        "#     return 1 - np.mean(feature_distances[:, 1:m+1], axis=1)\n",
        "\n",
        "# def calculate_kl_divergence(outputs, features, m=5):\n",
        "#     features_normalized = F.normalize(features, p=2, dim=1)\n",
        "#     feature_distances = pairwise_distances(features_normalized.cpu().detach().numpy(), metric='cosine')\n",
        "#     neighbor_indices = np.argsort(feature_distances, axis=1)[:, 1:m+1]\n",
        "#     kl_divergence = []\n",
        "\n",
        "#     for i in range(len(outputs)):\n",
        "#         current_sample_prob = F.softmax(outputs[i], dim=0)\n",
        "#         neighbors_probs = F.softmax(outputs[neighbor_indices[i]], dim=1)\n",
        "#         mean_neighbors_prob = torch.mean(neighbors_probs, dim=0)\n",
        "#         kl_div = F.kl_div(\n",
        "#             torch.log(torch.clamp(current_sample_prob, min=1e-10)),\n",
        "#             torch.clamp(mean_neighbors_prob, min=1e-10),\n",
        "#             reduction='batchmean'\n",
        "#         )\n",
        "#         kl_divergence.append(kl_div)\n",
        "\n",
        "#     return torch.stack(kl_divergence).cpu().numpy()\n",
        "\n",
        "# Cosine Similarity\n",
        "\n",
        "# model = CNNModel()\n",
        "# model.eval()\n",
        "# labeled_indices = active_learning_iteration(\n",
        "#     model, train_dataset, cosine_similarity, num_iterations, query_size, epochs_per_iteration\n",
        "# )\n",
        "# test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "# total = 0\n",
        "# correct = 0\n",
        "# with torch.no_grad():\n",
        "#     for images, labels in test_loader:\n",
        "#         outputs,_ = model(images)\n",
        "#         _, predicted = torch.max(outputs.data, 1)\n",
        "#         total += labels.size(0)\n",
        "#         correct += (predicted == labels).sum().item()\n",
        "\n",
        "# print(f\"Test Accuracy: {100 * correct / total:.2f}%\")\n",
        "# print(f\"Total Training Instances Used :\" ,labeled_indices)\n",
        "\n",
        "# KL - Divergence\n",
        "\n",
        "# model = CNNModel()\n",
        "# model.eval()\n",
        "# labeled_indices = active_learning_iteration(\n",
        "#     model, train_dataset, calculate_kl_divergence, num_iterations, query_size, epochs_per_iteration\n",
        "# )\n",
        "# test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "# total = 0\n",
        "# correct = 0\n",
        "# with torch.no_grad():\n",
        "#     for images, labels in test_loader:\n",
        "#         outputs = model(images)\n",
        "#         _, predicted = torch.max(outputs.data, 1)\n",
        "#         total += labels.size(0)\n",
        "#         correct += (predicted == labels).sum().item()\n",
        "\n",
        "# print(f\"Test Accuracy: {100 * correct / total:.2f}%\")\n",
        "# print(f\"Total Training Instances Used :\" ,labeled_indices)\n",
        "\n",
        "# L2 Norm\n",
        "\n",
        "# model = CNNModel()\n",
        "# model.eval()\n",
        "# labeled_indices = active_learning_iteration(\n",
        "#     model, train_dataset, L2, num_iterations, query_size, epochs_per_iteration\n",
        "# )\n",
        "# test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "# total = 0\n",
        "# correct = 0\n",
        "# with torch.no_grad():\n",
        "#     for images, labels in test_loader:\n",
        "#         outputs = model(images)\n",
        "#         _, predicted = torch.max(outputs.data, 1)\n",
        "#         total += labels.size(0)\n",
        "#         correct += (predicted == labels).sum().item()\n",
        "\n",
        "# print(f\"Test Accuracy: {100 * correct / total:.2f}%\")\n",
        "# print(f\"Total Training Instances Used :\" ,labeled_indices)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CnzFuROidhmW"
      },
      "source": [
        "Training the custom model fully on train dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "y1hp1M7mOe0z",
        "outputId": "2a06dbe4-9846-4cc6-c0b8-480dfb9708b2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Epoch 1/5, Loss: 1.3137108526310324\n",
            "Epoch 2/5, Loss: 0.9914887586571276\n",
            "Epoch 3/5, Loss: 0.8693068722894415\n",
            "Epoch 4/5, Loss: 0.7853797336323746\n",
            "Epoch 5/5, Loss: 0.7178750201885822\n"
          ]
        }
      ],
      "source": [
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=4, shuffle=False, num_workers=2)\n",
        "model = CNNModel()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "epochs = 5\n",
        "\n",
        "\n",
        "def calculate_diversity_metrics(features, m=5):\n",
        "    feature_distances = pairwise_distances(features.cpu().detach().numpy(), metric='cosine')\n",
        "    cosine_similarity = 1 - feature_distances[:, 1:m+1].mean(axis=1)\n",
        "    l2_distances = pairwise_distances(features.cpu().detach().numpy(), metric='euclidean')\n",
        "    l2_norm = l2_distances[:, 1:m+1].mean(axis=1)\n",
        "    return cosine_similarity, l2_norm\n",
        "\n",
        "\n",
        "def calculate_kl_divergence(outputs, feature_distances, m=5):\n",
        "    kl_divergence = []\n",
        "    for i in range(len(outputs)):\n",
        "        current_sample_prob = F.softmax(outputs[i], dim=0)\n",
        "        neighbor_indices = feature_distances[i, 1:m+1].astype(int)\n",
        "        neighbors_prob = torch.mean(F.softmax(outputs[neighbor_indices], dim=1), dim=0)\n",
        "        kl_divergence.append(F.kl_div(torch.log(current_sample_prob), neighbors_prob, reduction='batchmean'))\n",
        "\n",
        "    return kl_divergence\n",
        "\n",
        "def calculate_metrics(outputs, features, m=5):\n",
        "    features_normalized = F.normalize(features, p=2, dim=1)\n",
        "    cosine_similarity, l2_norm = calculate_diversity_metrics(features_normalized)\n",
        "    cosine_similarity_list.extend(torch.from_numpy(cosine_similarity))\n",
        "    l2_norm_list.extend(torch.from_numpy(l2_norm))\n",
        "    feature_distances = pairwise_distances(features.cpu().detach().numpy(), metric='cosine')\n",
        "    kl_divergence_scores = calculate_kl_divergence(outputs, feature_distances, m=5)\n",
        "    kl_divergence_list.extend(kl_divergence_scores)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        inputs, labels = data\n",
        "        optimizer.zero_grad()\n",
        "        outputs,_ = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss / len(trainloader)}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculating Accuracy of the fully trained custom model and its average Diversity Metrics"
      ],
      "metadata": {
        "id": "QuhvsUn56Hme"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "cosine_similarity_list = []\n",
        "l2_norm_list = []\n",
        "kl_divergence_list = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data\n",
        "        outputs, features = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "        calculate_metrics(outputs, features)\n",
        "\n",
        "accuracy = 100 * correct / total\n",
        "print(f\"Accuracy of fully trained custom model on trained set: {accuracy:.2f}%\")\n",
        "print(f\"Average Cosine Similarity: {torch.mean(torch.stack(cosine_similarity_list))}\")\n",
        "print(f\"Average L2 Norm: {torch.mean(torch.stack(l2_norm_list))}\")\n",
        "print(f\"Average KL Divergence: {torch.mean(torch.stack(kl_divergence_list))}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qkcWUGIh6Htn",
        "outputId": "f36162a9-c3d2-41c0-9c1c-f22760fd576e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of fully trained custom model on trained set: 68.14%\n",
            "Average Cosine Similarity: 0.4673972725868225\n",
            "Average L2 Norm: 0.8828698992729187\n",
            "Average KL Divergence: 0.45189204812049866\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.models import resnet50\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False, num_workers=2)\n",
        "\n",
        "model = resnet50(pretrained=True)\n",
        "\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = nn.Linear(num_ftrs, 10)\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "num_epochs = 5\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        inputs, labels = data[0].to(device), data[1].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "\n",
        "    print(f\"Epoch {epoch + 1},  Loss: {running_loss / 200:.4f}\")\n",
        "    running_loss = 0.0\n",
        "\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data[0].to(device), data[1].to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print('Accuracy of the RESNET50 : %d %%' % (100 * correct / total))"
      ],
      "metadata": {
        "id": "3fUu9iobzXN1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c368d1b-0372-4282-a403-f80600ccd938"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Epoch 1,  Loss: 3.0638\n",
            "Epoch 2,  Loss: 1.7790\n",
            "Epoch 3,  Loss: 1.3519\n",
            "Epoch 4,  Loss: 1.0774\n",
            "Epoch 5,  Loss: 0.8784\n",
            "Accuracy of the RESNET50 : 88 %\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}