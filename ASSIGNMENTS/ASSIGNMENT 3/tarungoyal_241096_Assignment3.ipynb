{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f556926f-e1d1-48e5-ad72-ee305a260d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "# Define Active Learning Strategies\n",
    "def least_confidence(model, data_loader, device):\n",
    "    model.eval()\n",
    "    uncertainties = []\n",
    "    with torch.no_grad():\n",
    "        for images, _ in data_loader:\n",
    "            images = images.to(device)\n",
    "            outputs = model(images)\n",
    "            probs = F.softmax(outputs, dim=1)\n",
    "            uncertainties.extend(1 - torch.max(probs, dim=1)[0].cpu().numpy())\n",
    "    return np.argsort(uncertainties)[::-1]  # Sort by least confidence\n",
    "\n",
    "def prediction_entropy(model, data_loader, device):\n",
    "    model.eval()\n",
    "    entropies = []\n",
    "    with torch.no_grad():\n",
    "        for images, _ in data_loader:\n",
    "            images = images.to(device)\n",
    "            outputs = model(images)\n",
    "            probs = F.softmax(outputs, dim=1)\n",
    "            entropy = -torch.sum(probs * torch.log(probs + 1e-6), dim=1).cpu().numpy()\n",
    "            entropies.extend(entropy)\n",
    "    return np.argsort(entropies)[::-1]  # Sort by highest entropy\n",
    "\n",
    "def margin_sampling(model, data_loader, device):\n",
    "    model.eval()\n",
    "    margins = []\n",
    "    with torch.no_grad():\n",
    "        for images, _ in data_loader:\n",
    "            images = images.to(device)\n",
    "            outputs = model(images)\n",
    "            probs = F.softmax(outputs, dim=1)\n",
    "            sorted_probs, _ = torch.sort(probs, descending=True)\n",
    "            margin = sorted_probs[:, 0] - sorted_probs[:, 1]\n",
    "            margins.extend(margin.cpu().numpy())\n",
    "    return np.argsort(margins)[::-1]  # Sort by largest margin\n",
    "\n",
    "def cosine_similarity_selection(model, data_loader, device, labeled_data):\n",
    "    model.eval()\n",
    "    labeled_embeddings = []\n",
    "    with torch.no_grad():\n",
    "        for images, _ in labeled_data:\n",
    "            images = images.to(device)\n",
    "            outputs = model(images)\n",
    "            labeled_embeddings.append(outputs.cpu().numpy())\n",
    "    labeled_embeddings = np.concatenate(labeled_embeddings, axis=0)\n",
    "\n",
    "    similarities = []\n",
    "    with torch.no_grad():\n",
    "        for images, _ in data_loader:\n",
    "            images = images.to(device)\n",
    "            outputs = model(images)\n",
    "            embeddings = outputs.cpu().numpy()\n",
    "            sim = cosine_similarity(embeddings, labeled_embeddings)\n",
    "            similarities.extend(np.max(sim, axis=1))\n",
    "    return np.argsort(similarities)[::-1]  # Sort by highest cosine similarity\n",
    "\n",
    "def l2_norm_selection(model, data_loader, device, labeled_data):\n",
    "    model.eval()\n",
    "    labeled_embeddings = []\n",
    "    with torch.no_grad():\n",
    "        for images, _ in labeled_data:\n",
    "            images = images.to(device)\n",
    "            outputs = model(images)\n",
    "            labeled_embeddings.append(outputs.cpu().numpy())\n",
    "    labeled_embeddings = np.concatenate(labeled_embeddings, axis=0)\n",
    "\n",
    "    distances = []\n",
    "    with torch.no_grad():\n",
    "        for images, _ in data_loader:\n",
    "            images = images.to(device)\n",
    "            outputs = model(images)\n",
    "            embeddings = outputs.cpu().numpy()\n",
    "            dist = cdist(embeddings, labeled_embeddings, 'euclidean')\n",
    "            distances.extend(np.min(dist, axis=1))\n",
    "    return np.argsort(distances)  # Sort by smallest L2 distance\n",
    "\n",
    "def kl_divergence_selection(model, data_loader, device):\n",
    "    model.eval()\n",
    "    divergences = []\n",
    "    with torch.no_grad():\n",
    "        for images, _ in data_loader:\n",
    "            images = images.to(device)\n",
    "            outputs = model(images)\n",
    "            probs = F.softmax(outputs, dim=1)\n",
    "            log_probs = F.log_softmax(outputs, dim=1)\n",
    "            divergence = torch.sum(probs * (torch.log(probs + 1e-6) - log_probs), dim=1).cpu().numpy()\n",
    "            divergences.extend(divergence)\n",
    "    return np.argsort(divergences)[::-1]  # Sort by largest KL divergence\n",
    "\n",
    "\n",
    "# Initialize models, criterion, optimizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "custom_model = CustomCNN(num_conv_layers=2, filter_size=16).to(device)\n",
    "pretrained_model = models.resnet18(pretrained=True)\n",
    "pretrained_model.fc = nn.Linear(pretrained_model.fc.in_features, 10)\n",
    "pretrained_model = pretrained_model.to(device)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "custom_optimizer = optim.Adam(custom_model.parameters(), lr=1e-3)\n",
    "pretrained_optimizer = optim.Adam(pretrained_model.parameters(), lr=1e-3)\n",
    "\n",
    "# Active learning loop with strategy tracking\n",
    "num_iterations = 5\n",
    "samples_per_iteration = 1000\n",
    "strategy_performance = {  # Track the average accuracy per strategy\n",
    "    'Least Confidence': [],\n",
    "    'Prediction Entropy': [],\n",
    "    'Margin Sampling': [],\n",
    "    'Cosine Similarity': [],\n",
    "    'L2 Norm': [],\n",
    "    'KL Divergence': []\n",
    "}\n",
    "\n",
    "# Function to train a model\n",
    "def train_model(model, data_loader, criterion, optimizer, device):\n",
    "    model.train()  # Set model to training mode\n",
    "    running_loss = 0.0\n",
    "    for images, labels in data_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate loss\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Return average loss for this epoch\n",
    "    epoch_loss = running_loss / len(data_loader)\n",
    "    return epoch_loss\n",
    "\n",
    "# Function to test the model\n",
    "def test_model(model, test_loader, device):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy\n",
    "\n",
    "# Active learning loop\n",
    "for iteration in range(num_iterations):\n",
    "    print(f\"\\nActive Learning Iteration {iteration + 1}/{num_iterations}\")\n",
    "\n",
    "    # Train custom model\n",
    "    for epoch in range(5):\n",
    "        train_loss = train_model(custom_model, labeled_loader, criterion, custom_optimizer, device)\n",
    "        print(f\"Custom Model - Iteration {iteration + 1}, Epoch {epoch + 1}, Loss: {train_loss:.4f}\")\n",
    "\n",
    "    custom_accuracy = test_model(custom_model, test_loader, device)\n",
    "    print(f\"Custom Model Test Accuracy after Iteration {iteration + 1}: {custom_accuracy:.2f}%\")\n",
    "\n",
    "    # Train pretrained model\n",
    "    for epoch in range(5):\n",
    "        train_loss = train_model(pretrained_model, labeled_loader, criterion, pretrained_optimizer, device)\n",
    "        print(f\"Pretrained Model - Iteration {iteration + 1}, Epoch {epoch + 1}, Loss: {train_loss:.4f}\")\n",
    "\n",
    "    pretrained_accuracy = test_model(pretrained_model, test_loader, device)\n",
    "    print(f\"Pretrained Model Test Accuracy after Iteration {iteration + 1}: {pretrained_accuracy:.2f}%\")\n",
    "\n",
    "    # Active learning strategy\n",
    "    if len(unlabeled_dataset) > 0:\n",
    "        unlabeled_loader = DataLoader(unlabeled_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        # Track accuracy for each strategy\n",
    "        strategy_results = {}\n",
    "\n",
    "        strategies = [\n",
    "            ('Least Confidence', least_confidence),\n",
    "            ('Prediction Entropy', prediction_entropy),\n",
    "            ('Margin Sampling', margin_sampling),\n",
    "            ('Cosine Similarity', cosine_similarity_selection),\n",
    "            ('L2 Norm', l2_norm_selection),\n",
    "            ('KL Divergence', kl_divergence_selection)\n",
    "        ]\n",
    "\n",
    "        for strategy_name, strategy_fn in strategies:\n",
    "            print(f\"Applying {strategy_name}...\")\n",
    "\n",
    "            selected_indices = strategy_fn(custom_model, unlabeled_loader, device, labeled_loader)\n",
    "            selected_indices = selected_indices[:samples_per_iteration]\n",
    "\n",
    "            labeled_indices = np.append(labeled_indices, unlabeled_indices[selected_indices])\n",
    "            unlabeled_indices = np.setdiff1d(unlabeled_indices, unlabeled_indices[selected_indices])\n",
    "\n",
    "            # Update datasets\n",
    "            labeled_dataset = Subset(train_dataset, labeled_indices)\n",
    "            unlabeled_dataset = Subset(train_dataset, unlabeled_indices)\n",
    "            labeled_loader = DataLoader(labeled_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "            # Test the model after applying this strategy\n",
    "            accuracy = test_model(custom_model, test_loader, device)\n",
    "            strategy_results[strategy_name] = accuracy\n",
    "            print(f\"{strategy_name} Test Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "        # Track performance for each strategy over all iterations\n",
    "        for strategy_name, accuracy in strategy_results.items():\n",
    "            strategy_performance[strategy_name].append(accuracy)\n",
    "\n",
    "# Compute average accuracy for each strategy across all iterations\n",
    "average_accuracy_per_strategy = {strategy: np.mean(accuracies) for strategy, accuracies in strategy_performance.items()}\n",
    "\n",
    "# Determine the most effective strategy overall\n",
    "most_effective_strategy = max(average_accuracy_per_strategy, key=average_accuracy_per_strategy.get)\n",
    "print(f\"\\nThe Most Effective Active Learning Strategy Overall: {most_effective_strategy}\")\n",
    "ChatGPT said:\n",
    "ChatGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078bfb2b-71ef-450a-8117-4898eb7b6f21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
