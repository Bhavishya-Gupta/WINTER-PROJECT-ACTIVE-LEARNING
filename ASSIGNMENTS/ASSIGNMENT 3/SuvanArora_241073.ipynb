{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zp8AUl4_aWKR"
      },
      "source": [
        "Core code for preparing dataset and forming the CNN architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UP0bsT1hZ90X"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn.functional as F\n",
        "\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "\n",
        "trainset = torchvision.datasets.MNIST('mnist_data', train=True, download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True)\n",
        "testset = torchvision.datasets.MNIST('mnist_data', train=False, download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=4, shuffle=False)\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 4 * 4, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 16 * 4 * 4)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = Net().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g0e2m2oZd-LT"
      },
      "source": [
        "Code for evaluating our CNN model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "m99xBOBYeAeK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74f5fd99-7ab3-439b-8870-73dbf520d45a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.1525, Accuracy: 95.14%\n",
            "Epoch 2, Loss: 0.0651, Accuracy: 98.12%\n",
            "Epoch 3, Loss: 0.0492, Accuracy: 98.53%\n",
            "Epoch 4, Loss: 0.0432, Accuracy: 98.72%\n",
            "Epoch 5, Loss: 0.0396, Accuracy: 98.90%\n",
            "Epoch 6, Loss: 0.0201, Accuracy: 99.40%\n",
            "Epoch 7, Loss: 0.0153, Accuracy: 99.56%\n",
            "Epoch 8, Loss: 0.0133, Accuracy: 99.61%\n",
            "Epoch 9, Loss: 0.0122, Accuracy: 99.63%\n",
            "Epoch 10, Loss: 0.0119, Accuracy: 99.67%\n",
            "The Accuracy Achieved in this CNN model is: 98.85%\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(10):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for inputs, labels in trainloader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    avg_loss = running_loss / len(trainloader)\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f\"Epoch {epoch+1}, Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n",
        "    scheduler.step()\n",
        "\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in testloader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "test_accuracy = 100 * correct / total\n",
        "print(f\"The Accuracy Achieved in this CNN model is: {test_accuracy:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJuWpd4AZqAs"
      },
      "source": [
        "Core Code for preparing the dataset for active learning model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "nM-JE0f_86zX"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import pairwise_distances\n",
        "from scipy.stats import entropy\n",
        "import numpy as np\n",
        "\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "\n",
        "trainset = torchvision.datasets.MNIST('mnist_data', train=True, download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True)\n",
        "testset = torchvision.datasets.MNIST('mnist_data', train=False, download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=4, shuffle=False)\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 4 * 4, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 16 * 4 * 4)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = Net().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tDkxZ9R6Zurn"
      },
      "source": [
        "Defining function for calculating uncertainty metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Y446gLW089e9"
      },
      "outputs": [],
      "source": [
        "def calculate_uncertainty_metrics(outputs):\n",
        "    outputs_tensor = torch.from_numpy(outputs)\n",
        "    probabilities = F.softmax(outputs_tensor, dim=1)\n",
        "\n",
        "    least_confidence = 1 - probabilities.max(dim=1).values.cpu().detach().numpy()\n",
        "\n",
        "    current_probs = probabilities.clone().detach()\n",
        "    current_probs[current_probs == 0] = 1e-10\n",
        "\n",
        "    prediction_entropy = -torch.sum(current_probs * torch.log(current_probs), dim=1).cpu().detach().numpy()\n",
        "\n",
        "    margin_sampling = 1 - (torch.max(probabilities, dim=1).values.cpu().detach().numpy() - \\\n",
        "                           torch.min(probabilities, dim=1).values.cpu().detach().numpy())\n",
        "\n",
        "    return least_confidence, prediction_entropy, margin_sampling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ayco75l4Zx_V"
      },
      "source": [
        "Defining function for calculating diversity metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "uSGV89cc8_oL"
      },
      "outputs": [],
      "source": [
        "def calculate_diversity_metrics(features, m=5):\n",
        "    feature_distances = pairwise_distances(features.cpu().detach().numpy(), metric='cosine')\n",
        "    cosine_similarity = 1 - feature_distances[:, 1:m+1].mean(axis=1)\n",
        "\n",
        "    l2_distances = pairwise_distances(features.cpu().detach().numpy(), metric='euclidean')\n",
        "    l2_norm = l2_distances[:, 1:m+1].mean(axis=1)\n",
        "\n",
        "    return cosine_similarity, l2_norm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQSl4rwpBUlv"
      },
      "source": [
        "Defining function for calculating KL divergence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "g8LYWimUBZsR"
      },
      "outputs": [],
      "source": [
        "def calculate_kl_divergence(outputs, feature_distances, m=5):\n",
        "    kl_divergence = []\n",
        "    for i in range(len(outputs)):\n",
        "        current_sample_prob = F.softmax(outputs[i], dim=0)\n",
        "        neighbor_indices = feature_distances[i, 1:m+1].astype(int)\n",
        "        neighbors_prob = torch.mean(F.softmax(outputs[neighbor_indices], dim=1), dim=0)\n",
        "        epsilon = 1e-10\n",
        "        current_sample_prob += epsilon\n",
        "        neighbors_prob += epsilon\n",
        "        kl_divergence.append(F.kl_div(torch.log(current_sample_prob), neighbors_prob, reduction='batchmean'))\n",
        "    return kl_divergence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bohvUBPgBhD3"
      },
      "source": [
        "Defining function for calculating metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "D-D7hZhaBeTD"
      },
      "outputs": [],
      "source": [
        "def calculate_metrics(outputs, features, m=5):\n",
        "    outputs_cpu = outputs.cpu()\n",
        "\n",
        "    least_confidence, prediction_entropy, margin_sampling = calculate_uncertainty_metrics(outputs_cpu.detach().numpy())\n",
        "    least_confidence_list.extend(torch.from_numpy(least_confidence))\n",
        "    prediction_entropy_list.extend(torch.from_numpy(prediction_entropy))\n",
        "    margin_sampling_list.extend(torch.from_numpy(margin_sampling))\n",
        "\n",
        "    features_normalized = F.normalize(features, p=2, dim=1)\n",
        "    cosine_similarity, l2_norm = calculate_diversity_metrics(features_normalized)\n",
        "\n",
        "    cosine_similarity_list.extend(torch.from_numpy(cosine_similarity))\n",
        "    l2_norm_list.extend(torch.from_numpy(l2_norm))\n",
        "\n",
        "    feature_distances = pairwise_distances(features.cpu().detach().numpy(), metric='cosine')\n",
        "    kl_divergence_scores = calculate_kl_divergence(outputs, feature_distances, m=5)\n",
        "    kl_divergence_list.extend(kl_divergence_scores)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_niE0fYXBUfF"
      },
      "source": [
        "Code for evaluating active learning enhanced model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KVhop_-ZB4rK",
        "outputId": "8caaa2bf-2b6a-4389-9b23-7a2b12d9bc56"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.2267, Accuracy: 93.14%\n",
            "Epoch 2, Loss: 0.1040, Accuracy: 97.06%\n",
            "Epoch 3, Loss: 0.0894, Accuracy: 97.51%\n",
            "Epoch 4, Loss: 0.0776, Accuracy: 97.78%\n",
            "Epoch 5, Loss: 0.0755, Accuracy: 97.97%\n",
            "Epoch 6, Loss: 0.0524, Accuracy: 98.56%\n",
            "Epoch 7, Loss: 0.0454, Accuracy: 98.73%\n",
            "Epoch 8, Loss: 0.0429, Accuracy: 98.81%\n",
            "Epoch 9, Loss: 0.0426, Accuracy: 98.86%\n",
            "Epoch 10, Loss: 0.0409, Accuracy: 98.83%\n",
            "Test Accuracy: 98.80%\n",
            "Average Least Confidence: 0.007506141439080238\n",
            "Average Prediction Entropy: 0.023189278319478035\n",
            "Average Margin Sampling: 0.007519974373281002\n",
            "Average Cosine Similarity: 0.7622093558311462\n",
            "Average L2 Norm: 0.5898517370223999\n",
            "Average KL Divergence: 1.379577875137329\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(10):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for inputs, labels in trainloader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    avg_loss = running_loss / len(trainloader)\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f\"Epoch {epoch+1}, Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n",
        "    scheduler.step()\n",
        "\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "least_confidence_list= []\n",
        "prediction_entropy_list = []\n",
        "margin_sampling_list = []\n",
        "cosine_similarity_list = []\n",
        "l2_norm_list = []\n",
        "kl_divergence_list = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in testloader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "        conv1_output = model.conv1(inputs)\n",
        "        relu_output = model.relu(conv1_output)\n",
        "        maxpool_output = model.pool(relu_output)\n",
        "        conv2_output = model.conv2(maxpool_output)\n",
        "        features = model.flatten(conv2_output)\n",
        "        features = features.view(features.size(0), -1)\n",
        "        calculate_metrics(outputs, features)\n",
        "\n",
        "accuracy = correct / total\n",
        "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "print(f\"Average Least Confidence: {torch.mean(torch.stack(least_confidence_list))}\")\n",
        "print(f\"Average Prediction Entropy: {torch.mean(torch.stack(prediction_entropy_list))}\")\n",
        "print(f\"Average Margin Sampling: {torch.mean(torch.stack(margin_sampling_list))}\")\n",
        "print(f\"Average Cosine Similarity: {torch.mean(torch.stack(cosine_similarity_list))}\")\n",
        "print(f\"Average L2 Norm: {torch.mean(torch.stack(l2_norm_list))}\")\n",
        "print(f\"Average KL Divergence: {torch.mean(torch.stack(kl_divergence_list))}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}