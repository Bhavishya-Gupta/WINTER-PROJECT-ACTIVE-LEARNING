{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from sklearn.metrics import pairwise_distances\n",
        "import numpy as np\n",
        "\n",
        "# Data Preparation\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=4, shuffle=False, num_workers=2)\n",
        "\n",
        "# CNN Model\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 16, 3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(32, 64, 3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.fc1 = nn.Linear(64 * 3 * 3, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = self.pool(F.relu(self.conv3(x)))\n",
        "        x = x.view(-1, 64 * 3 * 3)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Device Setup\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "net = Net().to(device)\n",
        "\n",
        "# Loss and Optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Training Loop\n",
        "for epoch in range(5):\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        inputs, labels = data[0].to(device), data[1].to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "        if i % 2000 == 1999:\n",
        "            print(f'[Epoch: {epoch + 1}, Batch: {i + 1}] loss: {running_loss / 2000:.4f}')\n",
        "            running_loss = 0.0\n",
        "\n",
        "print('Finished Training')\n",
        "\n",
        "# Metric Calculation Functions\n",
        "def calculate_metrics(outputs, features, m=5):\n",
        "    outputs_cpu = outputs.detach().cpu()\n",
        "    least_confidence, prediction_entropy, margin_sampling = calculate_uncertainty_metrics(outputs_cpu)\n",
        "    least_confidence_list.extend(least_confidence)\n",
        "    prediction_entropy_list.extend(prediction_entropy)\n",
        "    margin_sampling_list.extend(margin_sampling)\n",
        "\n",
        "    features_normalized = F.normalize(features, p=2, dim=1)\n",
        "    cosine_similarity, l2_norm = calculate_diversity_metrics(features_normalized, m)\n",
        "    cosine_similarity_list.extend(cosine_similarity)\n",
        "    l2_norm_list.extend(l2_norm)\n",
        "\n",
        "    feature_distances = pairwise_distances(features.cpu().numpy(), metric='cosine')\n",
        "    kl_divergence_scores = calculate_kl_divergence(outputs_cpu, feature_distances, m)\n",
        "    kl_divergence_list.extend(kl_divergence_scores)\n",
        "\n",
        "def calculate_uncertainty_metrics(outputs):\n",
        "    probabilities = F.softmax(outputs, dim=1)\n",
        "    least_confidence = 1 - probabilities.max(dim=1).values.cpu().numpy()\n",
        "    prediction_entropy = -torch.sum(probabilities * torch.log(probabilities + 1e-10), dim=1).cpu().numpy()\n",
        "    margin_sampling = torch.topk(probabilities, 2, dim=1).values.cpu().numpy()\n",
        "    margin_sampling = 1 - (margin_sampling[:, 0] - margin_sampling[:, 1])\n",
        "    return least_confidence, prediction_entropy, margin_sampling\n",
        "\n",
        "def calculate_diversity_metrics(features, m=5):\n",
        "    feature_distances = pairwise_distances(features.cpu().numpy(), metric='cosine')\n",
        "    cosine_similarity = 1 - feature_distances[:, 1:m+1].mean(axis=1)\n",
        "    l2_distances = pairwise_distances(features.cpu().numpy(), metric='euclidean')\n",
        "    l2_norm = l2_distances[:, 1:m+1].mean(axis=1)\n",
        "    return cosine_similarity, l2_norm\n",
        "\n",
        "def calculate_kl_divergence(outputs, feature_distances, m=5):\n",
        "    kl_divergence = []\n",
        "    for i in range(len(outputs)):\n",
        "        current_sample_prob = F.softmax(outputs[i], dim=0)\n",
        "        neighbor_indices = np.argsort(feature_distances[i])[:m+1]\n",
        "        neighbors_prob = torch.mean(F.softmax(outputs[neighbor_indices], dim=1), dim=0)\n",
        "        epsilon = 1e-10\n",
        "        kl_divergence.append(F.kl_div(torch.log(current_sample_prob + epsilon), neighbors_prob + epsilon, reduction='batchmean').item())\n",
        "    return kl_divergence\n",
        "\n",
        "# Evaluation and Metric Computation\n",
        "least_confidence_list = []\n",
        "prediction_entropy_list = []\n",
        "margin_sampling_list = []\n",
        "cosine_similarity_list = []\n",
        "l2_norm_list = []\n",
        "kl_divergence_list = []\n",
        "\n",
        "net.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for images, labels in testloader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = net(images)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "        # Feature extraction for metrics\n",
        "        features = net.pool(F.relu(net.conv1(images)))\n",
        "        features = net.pool(F.relu(net.conv2(features)))\n",
        "        features = net.pool(F.relu(net.conv3(features)))\n",
        "        features = features.view(features.size(0), -1)\n",
        "\n",
        "        calculate_metrics(outputs, features)\n",
        "\n",
        "# Results\n",
        "print(f'Test Accuracy: {100 * correct / total:.2f}%')\n",
        "print(f'Average Least Confidence: {np.mean(least_confidence_list):.4f}')\n",
        "print(f'Average Prediction Entropy: {np.mean(prediction_entropy_list):.4f}')\n",
        "print(f'Average Margin Sampling: {np.mean(margin_sampling_list):.4f}')\n",
        "print(f'Average Cosine Similarity: {np.mean(cosine_similarity_list):.4f}')\n",
        "print(f'Average L2 Norm: {np.mean(l2_norm_list):.4f}')\n",
        "print(f'Average KL Divergence: {np.mean(kl_divergence_list):.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Q3KkjCgvc6J",
        "outputId": "a0ed276c-2e67-46f5-dfe2-a85c9b273320"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch: 1, Batch: 2000] loss: 1.2433\n",
            "[Epoch: 1, Batch: 4000] loss: 0.6302\n",
            "[Epoch: 1, Batch: 6000] loss: 0.5424\n",
            "[Epoch: 1, Batch: 8000] loss: 0.4703\n",
            "[Epoch: 1, Batch: 10000] loss: 0.4402\n",
            "[Epoch: 1, Batch: 12000] loss: 0.3957\n",
            "[Epoch: 1, Batch: 14000] loss: 0.3707\n",
            "[Epoch: 2, Batch: 2000] loss: 0.3458\n",
            "[Epoch: 2, Batch: 4000] loss: 0.3255\n",
            "[Epoch: 2, Batch: 6000] loss: 0.3388\n",
            "[Epoch: 2, Batch: 8000] loss: 0.3253\n",
            "[Epoch: 2, Batch: 10000] loss: 0.3189\n",
            "[Epoch: 2, Batch: 12000] loss: 0.3078\n",
            "[Epoch: 2, Batch: 14000] loss: 0.3097\n",
            "[Epoch: 3, Batch: 2000] loss: 0.2827\n",
            "[Epoch: 3, Batch: 4000] loss: 0.2670\n",
            "[Epoch: 3, Batch: 6000] loss: 0.2789\n",
            "[Epoch: 3, Batch: 8000] loss: 0.2659\n",
            "[Epoch: 3, Batch: 10000] loss: 0.2732\n",
            "[Epoch: 3, Batch: 12000] loss: 0.2593\n",
            "[Epoch: 3, Batch: 14000] loss: 0.2697\n",
            "[Epoch: 4, Batch: 2000] loss: 0.2379\n",
            "[Epoch: 4, Batch: 4000] loss: 0.2563\n",
            "[Epoch: 4, Batch: 6000] loss: 0.2365\n",
            "[Epoch: 4, Batch: 8000] loss: 0.2356\n",
            "[Epoch: 4, Batch: 10000] loss: 0.2394\n",
            "[Epoch: 4, Batch: 12000] loss: 0.2361\n",
            "[Epoch: 4, Batch: 14000] loss: 0.2457\n",
            "[Epoch: 5, Batch: 2000] loss: 0.2131\n",
            "[Epoch: 5, Batch: 4000] loss: 0.2208\n",
            "[Epoch: 5, Batch: 6000] loss: 0.2231\n",
            "[Epoch: 5, Batch: 8000] loss: 0.2235\n",
            "[Epoch: 5, Batch: 10000] loss: 0.2248\n",
            "[Epoch: 5, Batch: 12000] loss: 0.2222\n",
            "[Epoch: 5, Batch: 14000] loss: 0.2150\n",
            "Finished Training\n",
            "Test Accuracy: 90.10%\n",
            "Average Least Confidence: 0.0807\n",
            "Average Prediction Entropy: 0.2154\n",
            "Average Margin Sampling: 0.1429\n",
            "Average Cosine Similarity: 0.5648\n",
            "Average L2 Norm: 0.8012\n",
            "Average KL Divergence: 0.6791\n"
          ]
        }
      ]
    }
  ]
}