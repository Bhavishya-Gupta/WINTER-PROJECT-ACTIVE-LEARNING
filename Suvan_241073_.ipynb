{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Importing Libraries"
      ],
      "metadata": {
        "id": "YqEt2BYFR_kO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "slZnjqR_Ref1"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preparing the dataset"
      ],
      "metadata": {
        "id": "9ObDo8onSDxw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5,),(0.5,))])\n",
        "\n",
        "trainset = torchvision.datasets.MNIST(root = './data',train = True,download = True, transform = transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset,batch_size = 4,shuffle = True, num_workers = 2)\n",
        "\n",
        "testset = torchvision.datasets.MNIST(root = './data',train = False,download = True,transform = transform)\n",
        "testloader = torch.utils.data.DataLoader(testset,batch_size = 4, shuffle = False, num_workers = 2)\n",
        "\n",
        "classes = tuple(str(i) for i in range(10))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wFibV914RztK",
        "outputId": "6a3bff0e-aa8e-4272-f748-1e96644c924b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:02<00:00, 4.59MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 132kB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1.65M/1.65M [00:01<00:00, 1.25MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 10.4MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CNN Architecture"
      ],
      "metadata": {
        "id": "ZsXTS75NYiS9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Network(nn.Module) :\n",
        "  def __init__(self):\n",
        "    super(Network,self).__init__()\n",
        "\n",
        "    self.Conv1 = nn.Conv2d(1,32,kernel_size = 3,padding = 1)\n",
        "    self.bn1 = nn.BatchNorm2d(32)\n",
        "    self.Conv2 = nn.Conv2d(32,64,kernel_size = 3,padding = 1)\n",
        "    self.bn2 = nn.BatchNorm2d(64)\n",
        "    self.Conv3 = nn.Conv2d(64,128,kernel_size = 3, padding = 1)\n",
        "    self.bn3 = nn.BatchNorm2d(128)\n",
        "\n",
        "    self.pool = nn.MaxPool2d(2,2)\n",
        "    self.gap = nn.AdaptiveAvgPool2d((1,1))\n",
        "\n",
        "    self.fc = nn.Linear(128,10)\n",
        "    self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "  def forward(self,x):\n",
        "      x = self.pool(F.relu(self.bn1(self.Conv1(x))))\n",
        "      x = self.pool(F.relu(self.bn2(self.Conv2(x))))\n",
        "      x = self.pool(F.relu(self.bn3(self.Conv3(x))))\n",
        "      x = self.gap(x)\n",
        "      x = torch.flatten(x,1)\n",
        "      x = self.dropout(x)\n",
        "      x = self.fc(x)\n",
        "      return x\n",
        "\n",
        "net = Network()"
      ],
      "metadata": {
        "id": "q3OqOUULUYcv"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loss Function and Optimizer"
      ],
      "metadata": {
        "id": "cn2uFSRwet73"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW(net.parameters(), lr=0.0001)"
      ],
      "metadata": {
        "id": "xKVxEbSnbX8p"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training the Network"
      ],
      "metadata": {
        "id": "XQVhBCQ1fgmk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "net.to(device)\n",
        "for epoch in range(2):\n",
        "  net.train()\n",
        "  running_loss = 0.0\n",
        "\n",
        "  for i,data in enumerate(trainloader,0):\n",
        "    inputs,labels = data\n",
        "    inputs = inputs.to(device)\n",
        "    labels = labels.to(device)\n",
        "    optimizer.zero_grad()\n",
        "    outputs = net(inputs)\n",
        "    loss = criterion(outputs,labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    running_loss += loss.item()\n",
        "    if i % 2000 == 1999:\n",
        "      print(f\"[Epoch {epoch + 1}, Batch {i + 1}] Loss: {running_loss / 2000:.3f}\")\n",
        "      running_loss = 0.0\n",
        "print(\"Training Finished\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5TQx1E0KfjcG",
        "outputId": "1804f74e-79ce-479a-a29b-c9ad7fc40028"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 1, Batch 2000] Loss: 0.178\n",
            "[Epoch 1, Batch 4000] Loss: 0.169\n",
            "[Epoch 1, Batch 6000] Loss: 0.161\n",
            "[Epoch 1, Batch 8000] Loss: 0.152\n",
            "[Epoch 1, Batch 10000] Loss: 0.151\n",
            "[Epoch 1, Batch 12000] Loss: 0.133\n",
            "[Epoch 1, Batch 14000] Loss: 0.128\n",
            "[Epoch 2, Batch 2000] Loss: 0.125\n",
            "[Epoch 2, Batch 4000] Loss: 0.116\n",
            "[Epoch 2, Batch 6000] Loss: 0.098\n",
            "[Epoch 2, Batch 8000] Loss: 0.106\n",
            "[Epoch 2, Batch 10000] Loss: 0.109\n",
            "[Epoch 2, Batch 12000] Loss: 0.099\n",
            "[Epoch 2, Batch 14000] Loss: 0.099\n",
            "Training Finished\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing the Network"
      ],
      "metadata": {
        "id": "p42kl8c9kNmI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "net.eval()\n",
        "with torch.no_grad():\n",
        "  for data in testloader:\n",
        "    inputs,labels = data\n",
        "    inputs = inputs.to(device)\n",
        "    labels = labels.to(device)\n",
        "    outputs = net(inputs)\n",
        "    _, predicted = torch.max(outputs.data,1)\n",
        "\n",
        "    total += labels.size(0)\n",
        "    correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy of the network on the 10000 test images: {100 * correct / total:.2f} %')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wkQGXvTjiiKh",
        "outputId": "bcd980d0-13c9-406d-eb69-1bc9cab11ab8"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network on the 10000 test images: 98.71 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Optical Extension"
      ],
      "metadata": {
        "id": "KJTF5Rsw_m4G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import models\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomCrop(28, padding=4),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "net = models.resnet18(pretrained = True)\n",
        "net.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "num_ftrs = net.fc.in_features\n",
        "net.fc = nn.Linear(num_ftrs, 10)\n",
        "\n",
        "net.to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW(net.parameters(), lr=0.0001)\n",
        "\n",
        "for epoch in range(2):\n",
        "  net.train()\n",
        "  running_loss = 0.0\n",
        "\n",
        "  for i,data in enumerate(trainloader,0):\n",
        "    inputs,labels = data\n",
        "    inputs = inputs.to(device)\n",
        "    labels = labels.to(device)\n",
        "    optimizer.zero_grad()\n",
        "    outputs = net(inputs)\n",
        "    loss = criterion(outputs,labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    running_loss += loss.item()\n",
        "    if i % 2000 == 1999:\n",
        "      print(f\"[Epoch {epoch + 1}, Batch {i + 1}] Loss: {running_loss / 2000:.3f}\")\n",
        "      running_loss = 0.0\n",
        "print(\"Training Finished\")\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "net.eval()\n",
        "with torch.no_grad():\n",
        "  for data in testloader:\n",
        "    inputs,labels = data\n",
        "    inputs = inputs.to(device)\n",
        "    labels = labels.to(device)\n",
        "    outputs = net(inputs)\n",
        "    _, predicted = torch.max(outputs.data,1)\n",
        "\n",
        "    total += labels.size(0)\n",
        "    correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy of the network on the 10000 test images: {100 * correct / total:.2f} %')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zS9uuaRGnB5M",
        "outputId": "eb50982f-9138-410a-c121-c9111a6ecabc"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 1, Batch 2000] Loss: 1.292\n",
            "[Epoch 1, Batch 4000] Loss: 0.761\n",
            "[Epoch 1, Batch 6000] Loss: 0.533\n",
            "[Epoch 1, Batch 8000] Loss: 0.422\n",
            "[Epoch 1, Batch 10000] Loss: 0.363\n",
            "[Epoch 1, Batch 12000] Loss: 0.311\n",
            "[Epoch 1, Batch 14000] Loss: 0.270\n",
            "[Epoch 2, Batch 2000] Loss: 0.226\n",
            "[Epoch 2, Batch 4000] Loss: 0.212\n",
            "[Epoch 2, Batch 6000] Loss: 0.174\n",
            "[Epoch 2, Batch 8000] Loss: 0.168\n",
            "[Epoch 2, Batch 10000] Loss: 0.158\n",
            "[Epoch 2, Batch 12000] Loss: 0.148\n",
            "[Epoch 2, Batch 14000] Loss: 0.158\n",
            "Training Finished\n",
            "Accuracy of the network on the 10000 test images: 91.51 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Jf22B0RF_3ae"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}